[{"content":"Recently, the observability team was tasked with a latency reporting project. The key features included:\nDaily and monthly latency aggregations Support for various quantiles A reasonable data delay SLO (24 hours) and more. For the purpose of this blog, we will focus on the data processing aspect. We needed a system that could read latency data from a Kafka stream, partition the data by various attributes such as endpoints, and precompute hourly, daily, and weekly quantiles (e.g., p50, p99).\nWhy did we choose Flink? We chose Flink because of several key features:\nStateful processing: It allows us to maintain state across events. Built-in windowing support: Flink offers windowing functions (e.g., tumbling, sliding) that divide the data stream into finite subsets (windows) based on time or count. Scalability: It can handle high-throughput data streams, such as those from Kafka. Fault tolerance: Through state snapshots and checkpointing, Flink ensures resilience. Flink Playground Naturally, besides going through the flink documentation [insert link], I decided go to my usual playground of choice. Docker. Below is the architecture. Checkout the code and docker-compose.yml for more information. Below is the basic architecture.\ngraph TD; P[\"Python App (produces latency data)\"] --\u003e A[\"Kafka Broker (latency topic)\"] %% Python app produces random latency data and sends it to the Kafka broker A --\u003e B[\"Flink Pipeline\"] %% Kafka Broker receives latency messages produced by the Python app, which are consumed by the Flink pipeline for processing B --\u003e C[\"1-Minute Tumbling Window\"] %% The Flink Pipeline splits the data stream into a 1-minute tumbling window for short-term aggregation B --\u003e D[\"1-Hour Tumbling Window\"] %% The Flink Pipeline also splits the data stream into a 1-hour tumbling window for long-term aggregation C --\u003e E[\"P50/P99 Aggregation\"] %% Once the 1-minute window is over, the data is processed to calculate P50 and P99 latency metrics D --\u003e F[\"P50/P99 Aggregation\"] %% Once the 1-hour window is over, the data is also processed to calculate P50 and P99 latency metrics E --\u003e G[\"Storage System (P50/P99 results)\"] %% The aggregated results from the 1-minute window are sent to a storage system for further use F --\u003e G[\"Storage System (P50/P99 results)\"] %% The aggregated results from the 1-hour window are also sent to a storage system for further use So what impressed me? Flink\u0026rsquo;s brilliance lies in its ability to effortlessly handle the complexity of distributed stream processing with concise, expressive code. take this one line for example\n1 2 3 4 5 val oneMinuteWindows = latencyStream .keyBy({ it.timestamp / 60000 }, TypeInformation.of(object : TypeHint\u0026lt;Long\u0026gt;() {})) .window(TumblingProcessingTimeWindows.of(Time.minutes(1))) .process(LatencyAggregateProcessFunction()) .returns(TypeInformation.of(object : TypeHint\u0026lt;AggregateResult\u0026gt;() {})) Flink simplifies real-time data processing by abstracting away many low-level concerns while giving developers precise control over time, state, and computation. In this single line, Flink allows us to:\nPartition the stream: The keyBy function logically partitions the incoming latencyStream based on timestamps, so each partition processes its events independently, enabling scalability in distributed environments. Windowing: Using TumblingProcessingTimeWindows.of(Time.minutes(1)), Flink groups the data into 1-minute windows based on event processing time, making it easy to aggregate data over defined time intervals. Custom processing: The process function applies a user-defined LatencyAggregateProcessFunction for calculating latency metrics, allowing custom logic to be executed for each window. Type safety and efficiency: Using Flink’s TypeHint, we ensure type safety and help optimize serialization and deserialization in distributed environments, which enhances performance. This one-liner hides a ton of complexity — from fault tolerance to scaling — and Flink’s ability to marry simplicity with powerful distributed processing is where it shines. In just a few lines of code, you have a resilient, scalable, and efficient stream processing pipeline!\n","date":"2024-10-07T23:00:00-07:00","image":"https://hashimcolombowala.com/p/flink/cover_hu6307248181568134095.jpg","permalink":"https://hashimcolombowala.com/p/flink/","title":"What is flink and why should we care about it"},{"content":"Recently, we revamped the way we manage dependencies for our internal Kotlin library. We migrated observability dependency management to Gradle Catalog to simplify and streamline the process.\nWhat is the Kotlin Gradle Catalog? The Kotlin Gradle Catalog is a Gradle feature that allows you to manage dependencies and plugins in a structured and reusable way using a central libs.versions.toml file. It helps organize and simplify the management of library versions, plugins, and other dependencies in Kotlin or Java projects, making the build script cleaner and reducing duplication.\nKey Features Centralized Dependency Management:\nDefine all dependencies and their versions in a single libs.versions.toml file. Reuse these definitions across your project or in multiple subprojects in a multi-module setup. Readability: Replace hardcoded dependency strings in build.gradle.kts files with human-readable aliases.\nVersion Control: Manage all library versions in one place, making upgrades and maintenance easier.\nStandardization: Ensure consistent dependency versions across all modules in a project.\nStructure of libs.versions.toml The libs.versions.toml file resides in the gradle folder by convention and organizes dependencies into:\nVersion Definitions:\n1 2 3 [versions] kotlin = \u0026#34;1.9.0\u0026#34; coroutines = \u0026#34;1.7.2\u0026#34; Library Aliases:\n1 2 3 [libraries] kotlin-stdlib = { module = \u0026#34;org.jetbrains.kotlin:kotlin-stdlib\u0026#34;, version.ref = \u0026#34;kotlin\u0026#34; } coroutines-core = { module = \u0026#34;org.jetbrains.kotlinx:kotlinx-coroutines-core\u0026#34;, version.ref = \u0026#34;coroutines\u0026#34; } Plugins:\n1 2 [plugins] kotlin = { id = \u0026#34;org.jetbrains.kotlin.jvm\u0026#34;, version.ref = \u0026#34;kotlin\u0026#34; } How We Integrated Prometheus and OpenTelemetry with Gradle Catalog 1. Defining Dependencies in libs.versions.toml Here’s how we structured the dependencies for Prometheus and OpenTelemetry:\n1 2 3 4 5 6 7 8 9 10 [versions] kotlin = \u0026#34;1.9.0\u0026#34; prometheus = \u0026#34;0.16.0\u0026#34; otel = \u0026#34;1.30.0\u0026#34; [libraries] micrometer-core = { module = \u0026#34;io.micrometer:micrometer-core\u0026#34;, version = \u0026#34;1.11.4\u0026#34; } micrometer-registry-prometheus = { module = \u0026#34;io.micrometer:micrometer-registry-prometheus\u0026#34;, version = \u0026#34;1.11.4\u0026#34; } otel-sdk = { module = \u0026#34;io.opentelemetry:opentelemetry-sdk\u0026#34;, version.ref = \u0026#34;otel\u0026#34; } otel-exporter-otlp = { module = \u0026#34;io.opentelemetry:opentelemetry-exporter-otlp\u0026#34;, version.ref = \u0026#34;otel\u0026#34; } 2. Configuring build.gradle.kts With the dependencies defined in the catalog, integrating them into the build script was much simpler:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 plugins { id(\u0026#34;org.jetbrains.kotlin.jvm\u0026#34;) version libs.versions.kotlin application } dependencies { implementation(libs.micrometer.core) implementation(libs.micrometer.registry.prometheus) implementation(libs.otel.sdk) implementation(libs.otel.exporter.otlp) } application { mainClass.set(\u0026#34;com.example.DropwizardApp\u0026#34;) } 3. Using Prometheus and OpenTelemetry in the Application We integrated Micrometer to expose metrics for Prometheus and OpenTelemetry for distributed tracing. The setup involved:\nInitializing Prometheus metrics with Micrometer: 1 2 3 4 5 6 val prometheusRegistry = PrometheusMeterRegistry(PrometheusConfig.DEFAULT) prometheusRegistry.bindTo(JvmMemoryMetrics()) prometheusRegistry.bindTo(JvmGcMetrics()) // Register Prometheus servlet environment.admin().addServlet(\u0026#34;metrics\u0026#34;, MetricsServlet(prometheusRegistry.prometheusRegistry)).addMapping(\u0026#34;/metrics\u0026#34;) Enabling OpenTelemetry auto-instrumentation: 1 AutoConfiguredOpenTelemetrySdk.initialize() Why We Switched to Gradle Catalog Reduced Boilerplate: Eliminating verbose dependency declarations from build.gradle.kts made the build script cleaner and more maintainable.\nCentralized Updates: Upgrading library versions became easier since all versioning now resides in libs.versions.toml.\nConsistency Across Projects: The catalog ensured that multiple subprojects in our ecosystem used the same dependency versions.\nConclusion By switching to Gradle Catalog, we simplified dependency management and ensured consistency across our Kotlin projects. Integrating Prometheus and OpenTelemetry was seamless, allowing us to instrument our services with minimal effort.\n","date":"2024-12-13T22:00:00-07:00","permalink":"https://hashimcolombowala.com/p/kotlin-gradle/","title":"Moving to Kotlin Gradle Catalog to simplify Dependency Management in Kotlin projects"},{"content":"Introduction Recently a colleague of mine shared a design doc in which he mentioned\nCallsite configuration for logging is strongly preferred for \u0026hellip;\u0026hellip;\nNow this was the first time I thought about callsite vs logger configuration. So here we go.\nWhat is Callsite Configuration? Callsite configuration applies logging settings directly to the code location where the logging call occurs. For instance, if you have a line like:\nPython Example:\n1 logger.info(f\u0026#34;Order processed successfully: {order_id}\u0026#34;) JVM Example:\n1 logger.info(\u0026#34;Order processed successfully: {}\u0026#34;, orderId); Callsite configuration allows you to specify details such as:\nLog level (e.g., info, debug). Formatting of the message. Whether the log message should even be emitted. This method provides fine-grained control, making it easier to:\nAdjust logging behavior for specific scenarios. Suppress unnecessary logs from overwhelming your system. Tailor logs for better debugging or performance insights. Since callsite configuration operates at the source of the logging call, it takes precedence over broader logger settings.\nWhat is Logger Configuration? Logger configuration, on the other hand, applies rules to all logs emitted by a specific logger. This is typically managed through centralized configuration files, such as:\nsepia.yaml for Python applications. logback.xml for JVM-based applications. Logger configuration is often used for third-party libraries like Kafka or Redis, where you can\u0026rsquo;t modify the logging calls directly. It allows you to:\nSet log levels globally for a library (e.g., suppressing verbose logs from Redis or Kafka). Control output destinations (e.g., log files, console). Apply global formatting. While logger configuration is powerful, its broad scope can lead to excessive or insufficient logging. It\u0026rsquo;s most useful for managing logs in codebases outside your control.\nWhy is Callsite Configuration Preferred? Precision\nCallsite configuration lets you adjust logging behavior exactly where it matters. This ensures that logs are actionable and relevant, reducing noise.\nDebugging Efficiency\nFine-tuning logs at the source of an issue makes debugging faster. Developers can emit detailed logs for complex operations without cluttering the rest of the system.\nOverrides Broad Rules\nSince callsite configurations take precedence, they allow developers to bypass general logger settings when specific adjustments are needed.\nThird-Party Exceptions\nLogger configuration is a fallback for third-party libraries where callsite control isn’t possible. However, this should be the exception, not the norm.\nExample: Combining Callsite and Logger Configurations Python Example Imagine an application that uses Redis for caching. You might configure logs like this:\nCallsite Configuration:\n1 2 if logger.isEnabledFor(logging.DEBUG): logger.debug(f\u0026#34;Processing cache entry for key: {key}\u0026#34;) Logger Configuration (sepia.yaml):\n1 2 3 4 5 loggers: redis: level: WARN handlers: - console Here, callsite configuration manages log verbosity for your application code, while logger configuration ensures Redis’s internal logs don’t overwhelm the system.\nJVM Example Now imagine a Java application using Kafka for messaging. You might configure logs like this:\nCallsite Configuration:\n1 2 3 if (logger.isDebugEnabled()) { logger.debug(\u0026#34;Processing message with key: {}\u0026#34;, messageKey); } Logger Configuration (logback.xml):\n1 2 3 4 5 6 7 8 9 10 11 \u0026lt;configuration\u0026gt; \u0026lt;logger name=\u0026#34;org.apache.kafka\u0026#34; level=\u0026#34;WARN\u0026#34;/\u0026gt; \u0026lt;appender name=\u0026#34;CONSOLE\u0026#34; class=\u0026#34;ch.qos.logback.core.ConsoleAppender\u0026#34;\u0026gt; \u0026lt;encoder\u0026gt; \u0026lt;pattern\u0026gt;%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n\u0026lt;/pattern\u0026gt; \u0026lt;/encoder\u0026gt; \u0026lt;/appender\u0026gt; \u0026lt;root level=\u0026#34;INFO\u0026#34;\u0026gt; \u0026lt;appender-ref ref=\u0026#34;CONSOLE\u0026#34;/\u0026gt; \u0026lt;/root\u0026gt; \u0026lt;/configuration\u0026gt; Here, callsite configuration provides fine-grained control for your Kafka-related logs, while logger configuration keeps Kafka’s verbose logs at a manageable level.\nBest Practices for Logging Configurations Use Callsite Configurations for Your Code\nApply logging settings directly in your application code to maintain control and precision.\nFallback to Logger Configurations for Third-Party Libraries\nUse logger configurations sparingly to manage logs from libraries or systems you can’t modify.\nDocument Logging Policies\nMaintain clear documentation on when to use callsite versus logger configurations to ensure consistency across your codebase.\nMonitor and Adjust\nPeriodically review logs to ensure they provide actionable insights without overwhelming storage or analysis tools.\nResources Python Logging Documentation Logback Documentation 12-Factor App: Logs Redis Logging Configuration Guide Kafka Logging Configuration Guide ","date":"2024-12-03T15:00:00-07:00","permalink":"https://hashimcolombowala.com/p/callsite/","title":"Callsite vs Logger Configurations: Fine-Tuning Your Logging Strategy"},{"content":"Introduction Phillip Carter\u0026rsquo;s blog \u0026ldquo;The Observability CAP Theorem\u0026rdquo; explores the challenging balancing act of building and managing observability systems for high-traffic organizations, something which resonates with me. It explores the trade-offs between key features of observability—like cost, data detail, retention, and query efficiency—and how organizations have to prioritize based on their specific needs.\nOne big takeaway is that no system can excel at everything. For example, making data queries fast and efficient might mean sacrificing how much data you can store long-term or how detailed that data is.\nAnother point that stood out is how observability priorities change over time. What works today might need to evolve as your company grows, regulations change, or new tools emerge. With that in mind, I wanted to write some thoughts and takeaways from the blog—both to reflect on and as a guide for future decisions.\nProperties of Observability Quick and efficient real-time querying of your data Sufficient historical data availability per query, ranging from days to years Comprehensive access to relevant data for a specific context Cost-effective operations that align with budgetary expectations Trade-offs in Observability Systems Due to the challenges of managing large volumes of data, observability systems often make trade-offs to optimize for specific use cases or properties. These trade-offs include:\nComprehensive Metric Storage vs. Investigation Efficiency: Observability systems may prioritize the ability to cheaply store and observe metrics for extensive datasets, but this can come at the cost of making in-depth investigations into root causes more challenging.\nExtensive Log Retention vs. Query Costs: Systems may enable the retention of all log data, even in a user-managed cloud environment, but this often involves incurring costs for each query or analysis performed.\nFast Trace Queries vs. Sampling Trade-offs: To enable fast querying and analysis of trace data, systems may rely on effective sampling techniques, potentially reducing the completeness of the data available for analysis.\nUnified Data Ingestion vs. User Experience Fragmentation: Systems that allow all types of data to be sent to a single tool may result in inconsistent user experiences, depending on the type of data being analyzed.\nUniform Analysis Across Data Sources vs. Specialized Capabilities: Tools that offer consistent analysis and visualization for all data types may sacrifice advanced features optimized for specific kinds of telemetry data.\nPrioritizing Observability Properties for companies generating high volume traffic For experienced engineers in organizations, unsampled high volume data can be stored cost-effectively in long-term archives, priorities shift to emphasize real-time operational efficiency. The updated order of importance is as follows:\nFast Queries for Incident Response\nQuick access to actionable insights during incidents is critical for minimizing downtime and addressing urgent issues. Engineers value systems that enable rapid querying over recent, relevant data, as this directly impacts the ability to resolve problems efficiently. Efficient Sampling and Aggregation\nEffective techniques like sampling, filtering, and aggregation allow teams to maintain representativeness while reducing data volume and cost. This ensures that a smaller, relevant subset of data is readily available in fast-querying stores, while the rest can be archived for later use if needed. Cost-Effective Compliance Data Retention\nCompliance data can be stored in low-cost archival systems (e.g., object storage like AWS S3 or similar solutions), minimizing its impact on operational observability costs. Retrieval and processing can be deferred to meet regulatory requirements without affecting real-time observability needs. Cost-Conscious Operations\nObservability costs must align with organizational budgets. Prioritizing cost-effective tools and strategies ensures observability delivers value without overextending financial resources. Open-source solutions or tiered pricing models for commercial tools can help manage costs effectively. Conclusion Observability handles different types of data streams with unpredictable volumes. The trade-offs in observability systems are rooted in technical and organizational realities. The ultimate impact of these trade-offs often depends on an organization’s unique priorities, such as tolerance for incident duration, audit requirements, tool standardization, and budget constraints. These priorities can shift over time due to internal dynamics or external pressures.\n","date":"2024-11-29T08:00:00-07:00","permalink":"https://hashimcolombowala.com/p/tradeoffs/","title":"Trade Offs made in o11y systems for high volume traffic"},{"content":"When it comes to collecting and analyzing metrics, one crucial decision is whether to use delta or cumulative metrics. These two approaches define how measurements are reported over time, and different metrics systems have their preferences based on their design and use cases. In this blog post, we’ll explore the difference between delta and cumulative metrics, examine their trade-offs, and highlight the systems that prefer one over the other.\nUnderstanding Temporality Temporality refers to the way metrics are recorded and reported over time. Choosing the right temporality depends on your monitoring backend and analytical needs.\nCumulative Temporality: Reports the running total of a metric from the beginning of a measurement period. Suitable for tracking trends and totals. Example: Total bytes transferred over time (source).\nDelta Temporality: Reports the difference in value for a metric since the last measurement. Ideal for real-time insights into changes over intervals. Example: Number of requests processed per interval (source).\nOpenTelemetry provides flexibility by supporting both delta and cumulative metrics. This allows observability platforms to choose a model that aligns with their architecture (learn more).\nCumulative Metrics:\nThese represent the total value accumulated from the start of the measurement up to the current point. Every reported value includes all previous measurements, providing a running total. Example: At time t0: 5 At time t0+10s: 10 At time t0+20s: 20 Prometheus documentation explains how cumulative counters are processed. Use case: Cumulative metrics are ideal when you need a holistic view of total occurrences, such as the total number of requests or errors since the application started. They also help mitigate data loss because missed intervals can still be accounted for in subsequent reports. Delta Metrics:\nDelta metrics report only the change in value since the last measurement. Each value reflects the increment or decrement during a specific reporting interval. Example: At time t0: 5 At time t0+10s: 5 At time t0+20s: 10 Learn more about delta metrics and how they\u0026rsquo;re handled in OpenTelemetry. Use case: Delta metrics are great for analyzing the rate of change over specific intervals. They’re especially useful in systems designed for high-frequency reporting where capturing incremental updates is more efficient. Key Differences Between Delta and Cumulative Metrics Feature Cumulative Metrics Delta Metrics Representation Running total since the start Increment since the last report Data Loss Handling Resilient to missed intervals Data may be lost if intervals are missed Rate Calculation Requires backend calculations Directly provides the rate per interval Overhead More storage-intensive for long periods More efficient for short intervals Metrics Systems and Their Preferences Metrics Systems Using Cumulative Metrics Prometheus: Cumulative metrics are the default. Counters track the total value over time, and functions like rate() derive per-second rates.\nGoogle Cloud Monitoring: Uses cumulative metrics to represent continuous totals, like the number of requests served.\nAmazon CloudWatch: Designed around cumulative metrics, with built-in support for rate and delta calculations.\nMetrics Systems Using Delta Metrics OpenTelemetry (OTLP): Delta temporality is supported for certain metric types, allowing granular reporting over intervals.\nStatsD: Metrics like counters are reported as deltas, focusing on changes during the last interval for lightweight and efficient ingestion.\nDatadog: Prefers delta metrics for some integrations to provide detailed visibility into per-second rates.\nSystems Supporting Both Delta and Cumulative Metrics OpenTelemetry: Provides flexibility by allowing users to configure metrics as either cumulative or delta.\nGrafana Mimir and Cortex: While aligned with Prometheus\u0026rsquo; cumulative approach, these systems can process both formats.\nElastic Observability: Supports both, though cumulative metrics are more effective for long-term storage and trend analysis.\nSplunk Observability Cloud: Handles both delta and cumulative metrics, adapting to the data source requirements.\nChoosing the Right Temporality When deciding between delta and cumulative metrics, consider the following:\nSystem Compatibility: Some systems (e.g., Prometheus) work better with cumulative metrics, while others (e.g., StatsD) prefer deltas. Data Loss Resilience: Cumulative metrics are more forgiving when intervals are missed. Analysis Needs: Use delta metrics for fine-grained interval analysis and cumulative metrics for overall trends and totals. Further Reading and Sources OpenTelemetry Metric Data Model Prometheus Metric Types and Temporality Grafana Blog: Converting Delta to Cumulative Metrics Google Cloud Monitoring and Metric Temporality ","date":"2024-11-26T06:49:00-07:00","permalink":"https://hashimcolombowala.com/p/delta/","title":"Delta vs. Cumulative Metrics: Key Differences and System Preferences"},{"content":"Kubernetes is known for its power and flexibility in managing containerized applications, but did you know it can also be extended to orchestrate custom workflows unique to your organization\u0026rsquo;s needs? Enter Custom Resource Definitions (CRDs). CRDs allow you to define your own resource types, extending Kubernetes beyond its out-of-the-box functionality. This means that Kubernetes can manage not just Pods and Services, but also any other kind of resource that your team requires.\nBy defining a CRD, we essentially added a new table to Kubernetes\u0026rsquo; internal document store via the API server (etcd). This resource could now be interacted with just like any built-in Kubernetes object. Using tools like kubectl, we could create, update, and delete instances of this new resource, seamlessly integrating it with existing workflows.\nBy using CRDs and controllers, we can offer reusable services to our teams while avoiding the overhead of developing new APIs from scratch. The built-in Kubernetes ecosystem provides authentication, authorization, and even CLI tooling for free, significantly reducing development time and complexity.\nWhy Use CRDs?\nKubernetes controllers reconcile the desired state of resources with their actual state, and we can create custom controllers using tools as simple as Bash. Custom Resource Definitions (CRDs) allow you to extend Kubernetes to meet your organization\u0026rsquo;s specific needs, making it a powerful platform not just for containers, but for custom orchestration tasks. Instead of building new microservices, leveraging Kubernetes CRDs and controllers lets you create powerful, integrated solutions that are already compatible with Kubernetes\u0026rsquo; tooling and infrastructure. Consistency: Using CRDs means everything—even your own internal processes—can be treated like a Kubernetes resource, managed consistently with the rest of your cluster. Tooling Integration: CRDs get the same benefits as built-in Kubernetes objects. You can use kubectl to interact with them, and they integrate seamlessly with Kubernetes\u0026rsquo; authentication, authorization, and RBAC policies. Modular Infrastructure: By using custom resources, you can create more modular infrastructure, which makes scaling easier. Your LoadTest or CronTask controller can run independently of your other services. How CRDs Work CRDs work by allowing you to register new types of resources with the Kubernetes API server. The Kubernetes API server is the brain behind Kubernetes. It stores information about the cluster\u0026rsquo;s state, using etcd as an internal document store, and ensures that whatever you\u0026rsquo;ve defined—a number of Pods running, a particular service being exposed—matches reality. etcd functions as a distributed key-value store where all the cluster\u0026rsquo;s state information, including your custom resources, is stored. With CRDs, you get to extend the brain to understand new resources that you define.\nThe process is straightforward:\nDefine Your Custom Resource: You describe what your new resource looks like. This could include fields like method and endpoint for a LoadTest resource. Register It with the API Server: Using a YAML or JSON file, you apply your CRD to the Kubernetes cluster. This lets the API server know about your new resource type. Write Controllers: Controllers are programs that watch your resources and ensure the state matches what you\u0026rsquo;ve defined. For your LoadTest, this could mean watching for new requests and kicking off an actual load test when one appears. Once you\u0026rsquo;ve done this, your Kubernetes cluster now understands this new type of resource—and you can manage these resources using the same tools and commands as any other part of Kubernetes.\nA Simple Example: Load Testing as a CRD Imagine you want a way for your developers to request a load test easily—no extra tooling, no separate scripts, just a simple definition. You could define a LoadTest custom resource, which has fields for the test duration, endpoint to test, and type of HTTP request. Checkout this [Github repo](https://github.com/hash167/kubernetes_controller) for a controller written in bash which watches for events about the CRD.\nHere\u0026rsquo;s a simple LoadTest definition:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: loadtests.platform.mycompany.com spec: group: platform.mycompany.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: endpoint: type: string duration: type: string method: type: string scope: Namespaced names: plural: loadtests singular: loadtest kind: LoadTest shortNames: - lt The updated CRD definition can also be found in the GitHub repository here.\nAfter defining this CRD and applying it, your developers can create LoadTest resources just like they create Pods or Deployments. A controller you build (or use off the shelf) would then watch for LoadTest resources and initiate the load test accordingly.\nWhy Use CRDs? Consistency: Using CRDs means everything—even your own internal processes—can be treated like a Kubernetes resource, managed consistently with the rest of your cluster. Tooling Integration: CRDs get the same benefits as built-in Kubernetes objects. You can use kubectl to interact with them, and they integrate seamlessly with Kubernetes\u0026rsquo; authentication, authorization, and RBAC policies. Modular Infrastructure: By using custom resources, you can create more modular infrastructure, which makes scaling easier. Your LoadTest or CronTask controller can run independently of your other services. Wrap-Up CRDs let you harness the full power of Kubernetes beyond managing containers. By integrating custom resources into the Kubernetes API, you bring consistency, power, and scalability to your workflows.\nThe best way to understand CRDs is to experiment—try extending Kubernetes with something that matches your team\u0026rsquo;s needs. And remember, Kubernetes is a platform that can grow with you—so don\u0026rsquo;t be afraid to push its boundaries.\n","date":"2024-10-12T23:00:35-07:00","permalink":"https://hashimcolombowala.com/p/custom-resource-definitions-crds-and-extending-kubernetes/","title":"Custom Resource Definitions (CRDs) and Extending Kubernetes"},{"content":"The Problem We have observed several Prometheus counters showing false spikes (i.e., no matching increase in logs) that could correlate with nodes experiencing kernel panics and reboots.\nWhy Do Counters Spike? In Prometheus, counters are expected to be monotonic: they can never decrease. The only time a counter timeseries can decrease is if it is reset to zero (e.g., when a container restarts).\nThe PromQL functions increase() and rate() have special logic to handle counter resets like this. If a counter has a value of 100 and the next data point is 99, it is assumed that the timeseries was first reset to 0 and then incremented 99 more times. This would cause a jump of 99 in most graphs.\nBackground Our Python app is running on a Gunicorn server with multiple workers. This means multiple Python processes are serving requests because Python\u0026rsquo;s threading is restricted by the Global Interpreter Lock (GIL). Using multiprocessing is a good workaround. The app is running on a pod on a Kubernetes node. This app is instrumented with the Prometheus Python client in multiprocessing mode because each worker process runs independently and maintains separate metrics. Multiprocessing mode aggregates these metrics across all workers, ensuring that Prometheus scrapes produce accurate, unified data across the entire application, reflecting all requests handled by all workers. In multiprocessing mode, each process writes its metrics to a separate set of mmapped files. On scrape, the exporter web server reads all of these files and merges them (i.e., counters from process A and B are summed). In our Kubernetes setup, these files are stored in /tmp, which is mounted as a Kubernetes emptyDir volume in most of our workloads. How Are Reboots Causing Spikes? It turns out the emptyDir mounted to /tmp persists across container crashes.\nWe were able to exec into a pod that experienced a node reboot and confirmed that the filesystem timestamps in the metric files predated the node reboot by several days.\nThis means that after a reboot, pods are coming back with their old counter values. This would normally be fine as long as the node is not down for too long—the counter would just resume at the previous value and see no reset, as long as it hasn\u0026rsquo;t fallen out of the backend aggregator\u0026rsquo;s buffer (which has a 10-minute window in our setup).\nIn the backend, we drilled into the raw data points for the counter during one of the spikes and noticed it was incrementing and then decrementing by exactly one:\nTIMESTAMP VALUE 2024-09-23T05:05:05.37-04:00 72929 2024-09-23T05:05:35.37-04:00 72929 2024-09-23T05:06:05.37-04:00 72930 2024-09-23T05:06:35.371-04:00 72930 2024-09-23T05:07:05.371-04:00 72932 2024-09-23T05:07:35.37-04:00 72932 2024-09-23T05:08:05.37-04:00 72933 2024-09-23T05:10:59.487-04:00 72932 (decrease in counter) 2024-09-23T05:11:29.496-04:00 72932 2024-09-23T05:11:59.5-04:00 72932 2024-09-23T05:12:29.489-04:00 72932 Sequence of Events The application process increments a counter from n to n+1 and writes the value to the mmapped file. This writes to the Linux kernel\u0026rsquo;s page cache (and is not immediately flushed to disk). A scrape occurs. The multiprocess exporter opens and reads all files. The kernel sees some of the files are already in the page cache and skips reading them from disk. The scrape exports the counter as n+1. A kernel panic happens before the page cache is flushed to disk. The counter increment is lost. The node encounters a kernel panic and reboots. Since the shutdown was not graceful, pods remain assigned to the node, so after startup containers are restarted with the same pod names, etc. Since the pod name is the same, the emptyDir volume is reused, and the pod keeps the last counter value that was flushed to disk (n). A scrape occurs, and we export the counter with a value of n. Prometheus queries run increase([..., n+1, n]), which is interpreted as an increase of n, causing a spike. However, we have not attempted to reproduce this behavior to confirm this theory. Since this depends on the timing of the kernel writing the dirty page to disk and the kernel panic, it also makes sense that we would not see this behavior consistently with node restarts.\nHow Can We Fix It? While a fix for the node reboot issue has been identified, we can be more robust here. The simplest solution is to clear out the metric files in /tmp on startup. Prometheus is designed for this—counter resets are normal.\nWe could set the Prometheus multiproc directory to a memory-backed emptyDir volume (emptyDir.medium: Memory). This would naturally be cleared on node restart. This would make writes count against container memory instead. We could add an init container that runs rm $PROMETHEUS_MULTIPROC_DIR/*.db on startup. This might impact pod start time slightly but is the simplest solution. We could make the application delete $PROMETHEUS_MULTIPROC_DIR/*.db on startup. Conclusion So here we have it. An off-by-one (decrement to the count) can lead to an increment of 99. Who would have thought.\n","date":"2024-10-09T21:00:00-07:00","permalink":"https://hashimcolombowala.com/p/another-off-by-one-mostly-problem-and-prometheus-counter-spikes/","title":"Another Off-by-One (Mostly) Problem and Prometheus Counter Spikes"},{"content":"Imagine you\u0026rsquo;re writing a library to extend the prometheus python client and you needed to add some dynamic labels specific to the environment. Ideally you would add most of your labels at the collector end and avoid writing extensions.\nSay you need to inject some dynamic labels and you will need to at some point, we may want to extend the client.\nWhat are Generics? and why do we use them in python? Without sacrificing the inherent safety of a statically typed language, generic programming gives us primitives to declare “placeholder types” that allow us to focus less on the specific types that may be used or declared by other portions of the codebase, but rather focus on these higher-level patterns that can be consolidated into simpler declarations.\nPython has no built-in type checking. As long as a given Python program is syntactically valid, it will run, and issues like incompatible types will only surface at runtime. This forces the developer to ensure there is error handling in place to deal with such errors, and even with this, a common best practice is to use type hints combined with third-party linting tools to try to stay on top of issues like this.\nGenerics in my opinion is a programming stype for statically typed languages brought into python via type hints.\nSome code with comments 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from prometheus_client import Counter as _PromCounter from prometheus_client import Histogram as _PromHistogram from typing import TypeVar, Generics # a new generic type, bound to two types _MetricsTypeT = TypeVar(\u0026#39;_MetricsTypeT\u0026#39;, bound=[_PromCounter, _PromHistogram]) # base class of type generic(child class to pass type in) class _MetricsBase(Generic[_MetricsTypeT]): def __init__(self, label_names: Iterable[str]): self.default_labels: Dict[str] = get_default_labels() self.all_label_names: list = list(label_names) + list(self.default_labels.keys()) self._parent_metric: _MetricsTypeT = None # Provides the label functionality def labels(self, *labelargs, **labelkwargs) -\u0026gt; _MetricsTypeT: if labelargs: labelargs += tuple(self.default_labels.values()) return cast(_MetricsTypeT, self._parent_metric.labels(*labelargs)) labelkwargs.update(self.default_labels) return cast(_MetricsTypeT, self._parent_metric.labels(**labelkwargs)) # child class passing in type to base class via generics class Counter(_MetricsBase[_PromCounter]): def __init__(self, name, documentation, labelnames=()): super().__init__(label_names=labelnames) self._parent_metric = _PromCounter( name=name, documentation=documentation, labelnames=self.all_label_names) Complete code here\nThe final world In my project, I used generics programing to type my base class. My base clase _MetricsBase accepts a Generic type, passed in by classes(Counter, Histogram) inheritering from it. The common method label returns the generic type passed in from the child class. When we use third party linters like mypy, we get some of the controll of statically typed languages with python\n","date":"2022-07-02T14:30:00Z","permalink":"https://hashimcolombowala.com/p/learning-generics-by-extending-the-prometheus-python-client/","title":"Learning Generics by extending the prometheus python client"},{"content":"To effectively monitor and understand the performance of your applications and infrastructure, having a well-designed metrics system is crucial. Here are the key requirements and components for building a reliable metrics system.\nRequirements for a Metrics System Multidimensional Data Model: The metrics system should support a multidimensional data model that can be sliced and diced along different dimensions as defined by the service (e.g., instance, service, endpoint, method). Operational Simplicity: The system should be easy to operate and maintain, minimizing overhead and complexity. Scalable Data Collection: The system must support scalable data collection and offer a decentralized architecture, allowing independent teams to set up their own monitoring servers. Powerful Query Language: A powerful query language should be available to leverage the data model for alerting and graphing, enabling precise insights into system performance. Client Libraries Client libraries play an essential role in the metrics system:\nThey handle details like thread safety, bookkeeping, and producing the Prometheus text exposition format in response to HTTP requests. Since metrics-based monitoring doesn\u0026rsquo;t track individual events, client library memory usage doesn\u0026rsquo;t increase with more events. Instead, memory usage depends on the number of metrics being tracked. Instrumentation To effectively monitor different types of services, appropriate instrumentation methods must be used. Here are three common types of services and how they should be instrumented:\nOnline-Serving Systems For online-serving systems, such as web services, the RED Method is used. This method involves tracking:\nRequests: The count of incoming requests. Errors: The count of failed requests. Duration: The latency or response time of requests. For example, a cache might track these metrics for both overall performance and for cache misses that need to be recalculated or fetched from a backend.\nOffline-Serving Systems Offline-serving systems, such as log processors, usually batch up work and consist of multiple stages in a pipeline with queues in between. These systems run continuously, which distinguishes them from batch jobs. The USE Method is used for these types of systems:\nUtilization: How much of the system\u0026rsquo;s capacity is in use (e.g., how much work is in progress). Saturation: The amount of queued work and how much work is currently being processed. Errors: Any errors encountered during processing. Batch Jobs Batch jobs are processes that run at scheduled intervals. The key metrics for batch jobs include:\nRun Time: How long it took for the job to complete. Stage Duration: How long each stage of the job took to complete. Success Time: The time at which the job last succeeded. Alerts can be set for when the job hasn\u0026rsquo;t succeeded within a certain time frame.\nIdempotency for Batch Jobs: Idempotency is an important concept for batch jobs. It means that performing an operation more than once has the same effect as performing it only once, which is crucial for reliability and preventing unintended side effects.\n","date":"2022-07-01T01:33:40Z","permalink":"https://hashimcolombowala.com/p/designing-a-metrics-system-notes/","title":"Designing a metrics system notes"},{"content":"Python decorators are one of my favorite features—they allow you to extend the functionality of existing functions or methods in a reusable and elegant way. Over the years, I’ve found them invaluable for streamlining repetitive tasks across different functions in my projects. Below are some interesting examples I\u0026rsquo;ve worked on\nPrometheus Histogram Timing Decorators OpenTelemetry (OTel) Manual Span Decorators Common Use Cases for Python Decorators 1. Timing Functions with Prometheus Measuring the execution time of functions is a classic use case for decorators. While custom decorators can do the job, the Prometheus Python client library provides a built-in time decorator, making this task even simpler.\nUsing the Built-In time Decorator 1 2 3 4 5 6 7 8 9 10 11 12 from prometheus_client import Histogram REQUEST_LATENCY = Histogram(\u0026#39;request_latency_seconds\u0026#39;, \u0026#39;Time spent processing request\u0026#39;) @REQUEST_LATENCY.time() def process_request(data): # Simulate processing import time time.sleep(2) return f\u0026#34;Processed {data}\u0026#34; process_request(\u0026#34;example\u0026#34;) This time() decorator wraps the function, measures its execution time, and updates the histogram. It’s concise, readable, and integrates seamlessly with Prometheus.\nHow the time Decorator Works Internally Here’s a simplified example of how the time decorator is implemented in the Prometheus Python library:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 from time import time from functools import wraps class Histogram: def __init__(self, name, description): self.name = name self.description = description self.observations = [] def observe(self, value): self.observations.append(value) print(f\u0026#34;Observed {value} seconds for {self.name}\u0026#34;) def time(self): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): start_time = time() try: return func(*args, **kwargs) finally: duration = time() - start_time self.observe(duration) return wrapper return decorator This decorator is a great example of how reusable logic is encapsulated to ensure consistent behavior.\nCustom Timing Decorator If you need custom behavior beyond just timing, you can create your own decorator:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import time def timing_decorator(func): def wrapper(*args, **kwargs): start_time = time.time() result = func(*args, **kwargs) end_time = time.time() print(f\u0026#34;{func.__name__} executed in {end_time - start_time:.4f} seconds\u0026#34;) return result return wrapper @timing_decorator def slow_function(): time.sleep(2) print(\u0026#34;Finished slow_function\u0026#34;) slow_function() 2. Tracing Functions with OpenTelemetry When implementing distributed tracing, decorators can simplify the process of adding spans to functions. Here’s an example using OpenTelemetry:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 from opentelemetry.trace import get_tracer tracer = get_tracer(__name__) def otel_span_decorator(span_name): def decorator(func): @wraps(func) def wrapper(*args, **kwargs): with tracer.start_as_current_span(span_name): return func(*args, **kwargs) return wrapper return decorator @otel_span_decorator(\u0026#34;process_request_span\u0026#34;) def process_request(data): # Simulate request processing print(f\u0026#34;Processing {data}\u0026#34;) process_request(\u0026#34;example\u0026#34;) This approach ensures spans are consistently created and closed without manually repeating the tracing logic in each function.\nSide Note: Retaining Function Signatures One important thing to watch out for when working with decorators is that they can alter the original function\u0026rsquo;s signature. This affects tools like help(), inspect, or frameworks that rely on function metadata (e.g., Flask, FastAPI).\nTo retain the original signature, use functools.wraps:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 from functools import wraps def my_decorator(func): @wraps(func) def wrapper(*args, **kwargs): print(f\u0026#34;Calling {func.__name__}\u0026#34;) return func(*args, **kwargs) return wrapper @my_decorator def greet(name): \u0026#34;\u0026#34;\u0026#34;Greets the user by name.\u0026#34;\u0026#34;\u0026#34; return f\u0026#34;Hello, {name}!\u0026#34; print(greet.__name__) # Outputs: greet print(greet.__doc__) # Outputs: Greets the user by name. Tip: Always use @wraps when writing decorators to avoid unexpected behavior!\nAdditional Use Cases for Decorators Here are other practical examples of Python decorators which I\u0026rsquo;ve seen being used. I will update this space if I come across any.\nRetrying Functions: Handle transient errors with retries. Setting a Maximum Execution Time: Enforce timeouts using signal. Logging Functions: Log function calls, inputs, and outputs. Simple Debugging: Print function inputs and outputs during development. Validating Function Inputs/Outputs: Ensure inputs/outputs meet certain criteria. Waiting/Rate-Limiting: Add delays to prevent overloading APIs. Caching/Memoization: Use functools.lru_cache for expensive calculations. Handling Database Transactions: Ensure proper commit/rollback. Synchronization: Use locks to prevent race conditions. Authentication: Enforce access control for web applications. References Python Decorators Documentation Prometheus Python Client Library PEP 318: Decorators for Functions and Methods ","date":"2022-04-04T15:54:35-07:00","permalink":"https://hashimcolombowala.com/p/python-decorators-a-super-useful-feature/","title":"Python decorators: A super useful feature"},{"content":"A while back, a senior colleague of mine was migrating a service from session-based authentication to token-based authentication. That got me curious. I decided to dive deeper into the topic, experiment a bit, and see how these approaches work in practice.\nWhat’s the Difference? The key difference between session-based and token-based authentication lies in where the state lives:\nIn session-based authentication, the server maintains the state (session information). In token-based authentication, the state is encoded in the token and lives on the client side. Session-Based Authentication In session-based authentication, after a user logs in, the server creates a session and assigns it a session ID. This session ID is stored on the client side (usually in a cookie). For every subsequent request, the client sends the session ID back to the server. The server checks its session store to validate the user.\nHere’s a quick rundown of how it works with some Flask magic:\nExample: Session-Based Authentication in Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 from flask import Flask, request, session, jsonify app = Flask(__name__) app.secret_key = \u0026#39;supersecretkey\u0026#39; # Used to sign session cookies @app.route(\u0026#39;/login\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def login(): username = request.json.get(\u0026#39;username\u0026#39;) password = request.json.get(\u0026#39;password\u0026#39;) # Dummy credentials check if username == \u0026#39;user\u0026#39; and password == \u0026#39;pass\u0026#39;: session[\u0026#39;user_id\u0026#39;] = username # Store the user\u0026#39;s ID in the session return jsonify(message=\u0026#34;Login successful\u0026#34;), 200 return jsonify(message=\u0026#34;Invalid credentials\u0026#34;), 401 @app.route(\u0026#39;/protected\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def protected(): if \u0026#39;user_id\u0026#39; in session: return jsonify(message=f\u0026#34;Hello, {session[\u0026#39;user_id\u0026#39;]}!\u0026#34;), 200 return jsonify(message=\u0026#34;Unauthorized\u0026#34;), 401 @app.route(\u0026#39;/logout\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def logout(): session.pop(\u0026#39;user_id\u0026#39;, None) # Remove user ID from session return jsonify(message=\u0026#34;Logged out\u0026#34;), 200 if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) How It Works: When the user logs in, the server creates a session and stores the user_id. The client gets a session cookie, which is sent with every request. The server checks the session store to validate the user. It’s simple, but the server has to maintain the session state, which can get tricky when scaling.\nToken-Based Authentication Token-based authentication is all about moving the state to the client. After the user logs in, the server generates a token (often a JWT) that contains the user information. The client stores the token and sends it with every request. The server validates the token to authenticate the user.\nHere’s how I played around with it:\nExample: Token-Based Authentication in Python 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 import jwt from flask import Flask, request, jsonify app = Flask(__name__) SECRET_KEY = \u0026#39;supersecretkey\u0026#39; # Create a JWT def create_jwt(user_id): return jwt.encode({\u0026#39;user_id\u0026#39;: user_id}, SECRET_KEY, algorithm=\u0026#39;HS256\u0026#39;) # Decode a JWT def decode_jwt(token): try: return jwt.decode(token, SECRET_KEY, algorithms=[\u0026#39;HS256\u0026#39;]) except jwt.ExpiredSignatureError: return None except jwt.InvalidTokenError: return None @app.route(\u0026#39;/login\u0026#39;, methods=[\u0026#39;POST\u0026#39;]) def login(): username = request.json.get(\u0026#39;username\u0026#39;) password = request.json.get(\u0026#39;password\u0026#39;) # Dummy credentials check if username == \u0026#39;user\u0026#39; and password == \u0026#39;pass\u0026#39;: token = create_jwt(username) return jsonify(token=token), 200 return jsonify(message=\u0026#34;Invalid credentials\u0026#34;), 401 @app.route(\u0026#39;/protected\u0026#39;, methods=[\u0026#39;GET\u0026#39;]) def protected(): auth_header = request.headers.get(\u0026#39;Authorization\u0026#39;) if not auth_header or not auth_header.startswith(\u0026#39;Bearer \u0026#39;): return jsonify(message=\u0026#34;Unauthorized\u0026#34;), 401 token = auth_header.split(\u0026#34; \u0026#34;)[1] decoded = decode_jwt(token) if decoded: return jsonify(message=f\u0026#34;Hello, {decoded[\u0026#39;user_id\u0026#39;]}!\u0026#34;), 200 return jsonify(message=\u0026#34;Unauthorized\u0026#34;), 401 if __name__ == \u0026#39;__main__\u0026#39;: app.run(debug=True) How It Works: The server generates a token with the user\u0026rsquo;s information and sends it to the client. The client sends the token in the Authorization header for subsequent requests. The server validates the token using its secret key. This approach is stateless, meaning the server doesn’t need to maintain a session store. Perfect for scaling!\nComparing the Two Approaches Here’s a quick summary of what I learned:\nSession-Based Authentication The server maintains the session state. Easier to invalidate sessions. Great for single-domain applications where scaling isn’t a concern. Token-Based Authentication The client holds the state (in the form of a token). Scales well horizontally because the server is stateless. Ideal for distributed systems or APIs. ","date":"2021-06-18T12:33:40-07:00","permalink":"https://hashimcolombowala.com/p/comparing-session-based-and-token-based-authentication/","title":"Comparing Session-Based and Token-Based Authentication"},{"content":"When deploying a containerized application to a container management system like AWS Fargate, you tend to run your application from a shell script. Suppose your script looks like this\n1 2 3 set -o nounset set -x gunicorn --config config/gunicorn/$GUNICORN_CONFIG.py config.wsgi Here we are executing the gunicorn service with PID 1 when the container is deployed. Suppose we want to terminate the container with a docker stop \u0026lt;container_id\u0026gt;, the command will send a SIGTERM to the container. As the gunicorn process is PID 1, this signal is ignored.\nThe way to resolve this issue is to use exec before the command to start your application. The last line of the above shell script should be gunicorn --config config/gunicorn/$GUNICORN_CONFIG.py config.wsgi. A simple example is shown below\nt.py:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import signal import time got_signal = False def process_signal(signum, frame): global got_signal got_signal = True signal.signal(signal.SIGINT, process_signal) signal.signal(signal.SIGTERM, process_signal) while not got_signal: time.sleep(1) print(\u0026#34;looping...\u0026#34;) print(\u0026#34;Ended with signal.\u0026#34;) entry.sh:\n1 2 #!/usr/bin/env bash exec python ./t.py Dockerfile:\n1 2 3 4 5 FROM python:3.7 WORKDIR /usr/src/app COPY . . CMD [ \u0026#34;./entry.sh\u0026#34; ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 With the exec: $ docker run --rm -t signal looping... looping... looping... looping... looping... looping... Ended with signal. $ Without the exec: $ docker run --rm -t signal looping... looping... looping... looping... looping... looping... $ Both times, you can run docker stop in another window. If the container doesn’t stop within 10 seconds, then it’s killed.\nMost apps do not explicitely handle SIGTERM the way t.py does. If you replace t.py with\n1 2 3 4 5 6 7 import time while True: time.sleep(1) print(\u0026#34;looping...\u0026#34;) print(\u0026#34;Ended with signal.\u0026#34;) SIGINT will work with keyboard interrupt but docker stop does not because of PID=1 issue.\nIf we run docker with a --init option to force a non 1 PID, the docker stop works whether we use exec in the script or not.\nIn order to use this feature with AWS Fargate, make a small change to your AWS::ECS::TaskDefinition in your cloudformation as shown below\n1 2 3 4 5 6 ContainerDefinitions: - Name: \u0026lt;container_name\u0026gt; Image: \u0026lt;container_image\u0026gt; Essential: \u0026#34;true\u0026#34; linuxParameters: initProcessEnabled: \u0026#34;true\u0026#34; ","date":"2021-01-27T01:33:40Z","permalink":"https://hashimcolombowala.com/p/signalling-docker-containers/","title":"Signalling docker containers"},{"content":"In python3 all classes are new style classes, thus it is reasonable to refer to an objects type and its class interchangably\nType and Class 1 2 3 4 5 6 class Foo: pass \u0026gt;\u0026gt;\u0026gt; type(Foo) \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; Type is a metaclass of which classes are instances\nFoo is an instance of metaclass type type is an instance of type as well Type Metaclass A type metaclass is initialized with 3 arguments\nname: name of the class (name attribute) bases: a tuple of classnames that the class inherits from namespace: a dictionary contianing definitions of the class body (dict attribute of the class) Creating an abstract class manually with Metaclasses To understand metaclasses, we create an interface or abstract class implementation. Use from abc import ABC, abstractmethod when implementing something at work.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 # test.py # Decorator to add attribute to function def abstract_func(func): func.__isabstract__ = True return func # This is the metaclass inheriting from `type` class Interface(type): def __init__(self, name, bases, namespace): print(f\u0026#34;Init method initialized from {self}\u0026#34;) class_methods = getattr(self, \u0026#39;all_methods\u0026#39;) for base in bases: required_methods = getattr(base, \u0026#39;abstract_methods\u0026#39;) for method in required_methods: if method not in class_methods: msg = f\u0026#34;\u0026#34;\u0026#34;Can\u0026#39;t create abstract class {name}! {name} must implement abstract method {method} of class {base}!\u0026#34;\u0026#34;\u0026#34; raise TypeError(msg) def __new__(cls, name, bases, namespace): namespace[\u0026#39;abstract_methods\u0026#39;] = Interface._get_abstract_methods(namespace) namespace[\u0026#39;all_methods\u0026#39;] = Interface._get_all_methods(namespace) cls = super().__new__(cls, name, bases, namespace) return cls def _get_abstract_methods(namespace): ret = [] for name, val in namespace.items(): if callable(val) and getattr(val, \u0026#39;__isabstract__\u0026#39;, False): ret.append(name) return ret def _get_all_methods(namespace): ret = [] for name, val in namespace.items(): if callable(val): ret.append(name) return ret # the __calls__() function calls the __new__() and __init__() methods of the metaclass class NetworkInterface(metaclass=Interface): @abstract_func def connect(self): pass @abstract_func def transfer(self): pass # The object of this class will not be created # because of missing abstract method class TestNetwork(NetworkInterface): def __init__(self): print(f\u0026#34;Init method initialized from {self}\u0026#34;) def connect(self): pass # def transfer(self): # pass c = TestNetwork() \u0026gt;\u0026gt;\u0026gt; TypeError: Can\u0026#39;t create abstract class TestNetwork! TestNetwork must implement abstract method transfer of class \u0026lt;class \u0026#39;__main__.NetworkInterface\u0026#39;\u0026gt;! After uncommenting the method \u0026gt;\u0026gt;\u0026gt; python3 test.py Init method initialized from \u0026lt;class \u0026#39;__main__.NetworkInterface\u0026#39;\u0026gt; Init method initialized from \u0026lt;class \u0026#39;__main__.TestNetwork\u0026#39;\u0026gt; Init method called from \u0026lt;__main__.TestNetwork object at 0x7fc5e7659350\u0026gt; When we initialize TestNetwork, the following happens\nThe interface init method is called twice. Once when creating the NetworkInterface and TestNetwork class from the metaclass blueprint. In the Interface init method, we iterate over the list of abstract methods in the parent class and make sure each one is present in the current class. If we don\u0026rsquo;t find a method in the class with the same name, we raise an exception ","date":"2020-12-28T01:33:40Z","permalink":"https://hashimcolombowala.com/p/abstract-classes-using-metaclasses/","title":"Abstract classes using Metaclasses"},{"content":"The Go compiler can compile the Go source go with different go specs. Fo example, if you have installed go 1.14, you can compile your source with Go spec 1.13.\nThe rules for which version of the Go spec used during compilation appear to be If your source code is stored within the GOPATH (or you have disabled modules with GO111MODULE=off) then the version of the Go spec used to compile matches the version of the compiler you are using. ie if you have go 1.13(GOROOT points to your go installation) installed then the go spec used will be 1.13 If your source code is outside the GOPATH(or GO111MODULE=on), then the go tool will take the version from the go.mod file If no go.mod file is provided, then same as point 1 If you are in module mode(see point 2) and no version is specified in the go.mod file, then go 1.13 is used by default The last point is interesting.\nReference: https://www.jetbrains.com/help/go/configuring-goroot-and-gopath.html\n","date":"2020-06-20T16:00:00-07:00","permalink":"https://hashimcolombowala.com/p/the-go-speccompiling/","title":"The Go Spec(compiling)"},{"content":"Every once in a while, it\u0026rsquo;s good to revisit the principles that guide Python\u0026rsquo;s design and philosophy. One of the most iconic expressions of these principles is \u0026ldquo;The Zen of Python\u0026rdquo; by Tim Peters. If you haven\u0026rsquo;t read it in a while, it might be worth taking a moment to appreciate the simplicity and wisdom it contains.\nTo view \u0026ldquo;The Zen of Python,\u0026rdquo; open a Python interpreter and enter:\n1 \u0026gt;\u0026gt;\u0026gt; import this Here\u0026rsquo;s what you\u0026rsquo;ll see:\nThe Zen of Python, by Tim Peters\nBeautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\u0026rsquo;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one—and preferably only one—obvious way to do it. Although that way may not be obvious at first unless you\u0026rsquo;re Dutch. Now is better than never. Although never is often better than right now. If the implementation is hard to explain, it\u0026rsquo;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea—let\u0026rsquo;s do more of those! The Zen of Python encapsulates the values that make Python such a delightful programming language. It\u0026rsquo;s about simplicity, clarity, and pragmatism. Whether you\u0026rsquo;re new to Python or a seasoned developer, keeping these principles in mind can help guide you to write better, more maintainable code.\n","date":"2020-04-07T18:54:00-07:00","permalink":"https://hashimcolombowala.com/p/the-zen-of-python/","title":"The Zen of Python"}]