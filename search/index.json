[{"content":"Recently, the observability team was tasked with a latency reporting project. The key features included:\nDaily and monthly latency aggregations Support for various quantiles A reasonable data delay SLO (24 hours) and more. For the purpose of this blog, we will focus on the data processing aspect. We needed a system that could read latency data from a Kafka stream, partition the data by various attributes such as endpoints, and precompute hourly, daily, and weekly quantiles (e.g., p50, p99).\nWhy did we choose Flink? We chose Flink because of several key features:\nStateful processing: It allows us to maintain state across events. Built-in windowing support: Flink offers windowing functions (e.g., tumbling, sliding) that divide the data stream into finite subsets (windows) based on time or count. Scalability: It can handle high-throughput data streams, such as those from Kafka. Fault tolerance: Through state snapshots and checkpointing, Flink ensures resilience. Flink Playground Naturally, besides going through the flink documentation [insert link], I decided go to my usual playground of choice. Docker. Below is the architecture. Checkout the code and docker-compose.yml for more information. Below is the basic architecture.\ngraph TD; P[\"Python App (produces latency data)\"] --\u003e A[\"Kafka Broker (latency topic)\"] %% Python app produces random latency data and sends it to the Kafka broker A --\u003e B[\"Flink Pipeline\"] %% Kafka Broker receives latency messages produced by the Python app, which are consumed by the Flink pipeline for processing B --\u003e C[\"1-Minute Tumbling Window\"] %% The Flink Pipeline splits the data stream into a 1-minute tumbling window for short-term aggregation B --\u003e D[\"1-Hour Tumbling Window\"] %% The Flink Pipeline also splits the data stream into a 1-hour tumbling window for long-term aggregation C --\u003e E[\"P50/P99 Aggregation\"] %% Once the 1-minute window is over, the data is processed to calculate P50 and P99 latency metrics D --\u003e F[\"P50/P99 Aggregation\"] %% Once the 1-hour window is over, the data is also processed to calculate P50 and P99 latency metrics E --\u003e G[\"Storage System (P50/P99 results)\"] %% The aggregated results from the 1-minute window are sent to a storage system for further use F --\u003e G[\"Storage System (P50/P99 results)\"] %% The aggregated results from the 1-hour window are also sent to a storage system for further use So what impressed me? Flink\u0026rsquo;s brilliance lies in its ability to effortlessly handle the complexity of distributed stream processing with concise, expressive code. take this one line for example\n1 2 3 4 5 val oneMinuteWindows = latencyStream .keyBy({ it.timestamp / 60000 }, TypeInformation.of(object : TypeHint\u0026lt;Long\u0026gt;() {})) .window(TumblingProcessingTimeWindows.of(Time.minutes(1))) .process(LatencyAggregateProcessFunction()) .returns(TypeInformation.of(object : TypeHint\u0026lt;AggregateResult\u0026gt;() {})) Flink simplifies real-time data processing by abstracting away many low-level concerns while giving developers precise control over time, state, and computation. In this single line, Flink allows us to:\nPartition the stream: The keyBy function logically partitions the incoming latencyStream based on timestamps, so each partition processes its events independently, enabling scalability in distributed environments. Windowing: Using TumblingProcessingTimeWindows.of(Time.minutes(1)), Flink groups the data into 1-minute windows based on event processing time, making it easy to aggregate data over defined time intervals. Custom processing: The process function applies a user-defined LatencyAggregateProcessFunction for calculating latency metrics, allowing custom logic to be executed for each window. Type safety and efficiency: Using Flink’s TypeHint, we ensure type safety and help optimize serialization and deserialization in distributed environments, which enhances performance. This one-liner hides a ton of complexity — from fault tolerance to scaling — and Flink’s ability to marry simplicity with powerful distributed processing is where it shines. In just a few lines of code, you have a resilient, scalable, and efficient stream processing pipeline!\n","date":"2024-10-07T23:00:00-07:00","image":"https://hashimcolombowala.com/p/flink/cover_hu6307248181568134095.jpg","permalink":"https://hashimcolombowala.com/p/flink/","title":"What is flink and why should we care about it"},{"content":"Kubernetes is known for its power and flexibility in managing containerized applications, but did you know it can also be extended to orchestrate custom workflows unique to your organization\u0026rsquo;s needs? Enter Custom Resource Definitions (CRDs). CRDs allow you to define your own resource types, extending Kubernetes beyond its out-of-the-box functionality. This means that Kubernetes can manage not just Pods and Services, but also any other kind of resource that your team requires.\nBy defining a CRD, we essentially added a new table to Kubernetes\u0026rsquo; internal document store via the API server (etcd). This resource could now be interacted with just like any built-in Kubernetes object. Using tools like kubectl, we could create, update, and delete instances of this new resource, seamlessly integrating it with existing workflows.\nBy using CRDs and controllers, we can offer reusable services to our teams while avoiding the overhead of developing new APIs from scratch. The built-in Kubernetes ecosystem provides authentication, authorization, and even CLI tooling for free, significantly reducing development time and complexity.\nWhy Use CRDs?\nKubernetes controllers reconcile the desired state of resources with their actual state, and we can create custom controllers using tools as simple as Bash. Custom Resource Definitions (CRDs) allow you to extend Kubernetes to meet your organization\u0026rsquo;s specific needs, making it a powerful platform not just for containers, but for custom orchestration tasks. Instead of building new microservices, leveraging Kubernetes CRDs and controllers lets you create powerful, integrated solutions that are already compatible with Kubernetes\u0026rsquo; tooling and infrastructure. Consistency: Using CRDs means everything—even your own internal processes—can be treated like a Kubernetes resource, managed consistently with the rest of your cluster. Tooling Integration: CRDs get the same benefits as built-in Kubernetes objects. You can use kubectl to interact with them, and they integrate seamlessly with Kubernetes\u0026rsquo; authentication, authorization, and RBAC policies. Modular Infrastructure: By using custom resources, you can create more modular infrastructure, which makes scaling easier. Your LoadTest or CronTask controller can run independently of your other services. How CRDs Work CRDs work by allowing you to register new types of resources with the Kubernetes API server. The Kubernetes API server is the brain behind Kubernetes. It stores information about the cluster\u0026rsquo;s state, using etcd as an internal document store, and ensures that whatever you\u0026rsquo;ve defined—a number of Pods running, a particular service being exposed—matches reality. etcd functions as a distributed key-value store where all the cluster\u0026rsquo;s state information, including your custom resources, is stored. With CRDs, you get to extend the brain to understand new resources that you define.\nThe process is straightforward:\nDefine Your Custom Resource: You describe what your new resource looks like. This could include fields like method and endpoint for a LoadTest resource. Register It with the API Server: Using a YAML or JSON file, you apply your CRD to the Kubernetes cluster. This lets the API server know about your new resource type. Write Controllers: Controllers are programs that watch your resources and ensure the state matches what you\u0026rsquo;ve defined. For your LoadTest, this could mean watching for new requests and kicking off an actual load test when one appears. Once you\u0026rsquo;ve done this, your Kubernetes cluster now understands this new type of resource—and you can manage these resources using the same tools and commands as any other part of Kubernetes.\nA Simple Example: Load Testing as a CRD Imagine you want a way for your developers to request a load test easily—no extra tooling, no separate scripts, just a simple definition. You could define a LoadTest custom resource, which has fields for the test duration, endpoint to test, and type of HTTP request. Checkout this [Github repo](https://github.com/hash167/kubernetes_controller) for a controller written in bash which watches for events about the CRD.\nHere\u0026rsquo;s a simple LoadTest definition:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 apiVersion: apiextensions.k8s.io/v1 kind: CustomResourceDefinition metadata: name: loadtests.platform.mycompany.com spec: group: platform.mycompany.com versions: - name: v1 served: true storage: true schema: openAPIV3Schema: type: object properties: spec: type: object properties: endpoint: type: string duration: type: string method: type: string scope: Namespaced names: plural: loadtests singular: loadtest kind: LoadTest shortNames: - lt The updated CRD definition can also be found in the GitHub repository here.\nAfter defining this CRD and applying it, your developers can create LoadTest resources just like they create Pods or Deployments. A controller you build (or use off the shelf) would then watch for LoadTest resources and initiate the load test accordingly.\nWhy Use CRDs? Consistency: Using CRDs means everything—even your own internal processes—can be treated like a Kubernetes resource, managed consistently with the rest of your cluster. Tooling Integration: CRDs get the same benefits as built-in Kubernetes objects. You can use kubectl to interact with them, and they integrate seamlessly with Kubernetes\u0026rsquo; authentication, authorization, and RBAC policies. Modular Infrastructure: By using custom resources, you can create more modular infrastructure, which makes scaling easier. Your LoadTest or CronTask controller can run independently of your other services. Wrap-Up CRDs let you harness the full power of Kubernetes beyond managing containers. By integrating custom resources into the Kubernetes API, you bring consistency, power, and scalability to your workflows.\nThe best way to understand CRDs is to experiment—try extending Kubernetes with something that matches your team\u0026rsquo;s needs. And remember, Kubernetes is a platform that can grow with you—so don\u0026rsquo;t be afraid to push its boundaries.\n","date":"2024-10-12T23:00:35-07:00","permalink":"https://hashimcolombowala.com/p/custom-resource-definitions-crds-and-extending-kubernetes/","title":"Custom Resource Definitions (CRDs) and Extending Kubernetes"},{"content":"The Problem We have observed several Prometheus counters showing false spikes (i.e., no matching increase in logs) that could correlate with nodes experiencing kernel panics and reboots.\nWhy Do Counters Spike? In Prometheus, counters are expected to be monotonic: they can never decrease. The only time a counter timeseries can decrease is if it is reset to zero (e.g., when a container restarts).\nThe PromQL functions increase() and rate() have special logic to handle counter resets like this. If a counter has a value of 100 and the next data point is 99, it is assumed that the timeseries was first reset to 0 and then incremented 99 more times. This would cause a jump of 99 in most graphs.\nBackground Our Python app is running on a Gunicorn server with multiple workers. This means multiple Python processes are serving requests because Python\u0026rsquo;s threading is restricted by the Global Interpreter Lock (GIL). Using multiprocessing is a good workaround. The app is running on a pod on a Kubernetes node. This app is instrumented with the Prometheus Python client in multiprocessing mode because each worker process runs independently and maintains separate metrics. Multiprocessing mode aggregates these metrics across all workers, ensuring that Prometheus scrapes produce accurate, unified data across the entire application, reflecting all requests handled by all workers. In multiprocessing mode, each process writes its metrics to a separate set of mmapped files. On scrape, the exporter web server reads all of these files and merges them (i.e., counters from process A and B are summed). In our Kubernetes setup, these files are stored in /tmp, which is mounted as a Kubernetes emptyDir volume in most of our workloads. How Are Reboots Causing Spikes? It turns out the emptyDir mounted to /tmp persists across container crashes.\nWe were able to exec into a pod that experienced a node reboot and confirmed that the filesystem timestamps in the metric files predated the node reboot by several days.\nThis means that after a reboot, pods are coming back with their old counter values. This would normally be fine as long as the node is not down for too long—the counter would just resume at the previous value and see no reset, as long as it hasn\u0026rsquo;t fallen out of the backend aggregator\u0026rsquo;s buffer (which has a 10-minute window in our setup).\nIn the backend, we drilled into the raw data points for the counter during one of the spikes and noticed it was incrementing and then decrementing by exactly one:\nTIMESTAMP VALUE 2024-09-23T05:05:05.37-04:00 72929 2024-09-23T05:05:35.37-04:00 72929 2024-09-23T05:06:05.37-04:00 72930 2024-09-23T05:06:35.371-04:00 72930 2024-09-23T05:07:05.371-04:00 72932 2024-09-23T05:07:35.37-04:00 72932 2024-09-23T05:08:05.37-04:00 72933 2024-09-23T05:10:59.487-04:00 72932 (decrease in counter) 2024-09-23T05:11:29.496-04:00 72932 2024-09-23T05:11:59.5-04:00 72932 2024-09-23T05:12:29.489-04:00 72932 Sequence of Events The application process increments a counter from n to n+1 and writes the value to the mmapped file. This writes to the Linux kernel\u0026rsquo;s page cache (and is not immediately flushed to disk). A scrape occurs. The multiprocess exporter opens and reads all files. The kernel sees some of the files are already in the page cache and skips reading them from disk. The scrape exports the counter as n+1. A kernel panic happens before the page cache is flushed to disk. The counter increment is lost. The node encounters a kernel panic and reboots. Since the shutdown was not graceful, pods remain assigned to the node, so after startup containers are restarted with the same pod names, etc. Since the pod name is the same, the emptyDir volume is reused, and the pod keeps the last counter value that was flushed to disk (n). A scrape occurs, and we export the counter with a value of n. Prometheus queries run increase([..., n+1, n]), which is interpreted as an increase of n, causing a spike. However, we have not attempted to reproduce this behavior to confirm this theory. Since this depends on the timing of the kernel writing the dirty page to disk and the kernel panic, it also makes sense that we would not see this behavior consistently with node restarts.\nHow Can We Fix It? While a fix for the node reboot issue has been identified, we can be more robust here. The simplest solution is to clear out the metric files in /tmp on startup. Prometheus is designed for this—counter resets are normal.\nWe could set the Prometheus multiproc directory to a memory-backed emptyDir volume (emptyDir.medium: Memory). This would naturally be cleared on node restart. This would make writes count against container memory instead. We could add an init container that runs rm $PROMETHEUS_MULTIPROC_DIR/*.db on startup. This might impact pod start time slightly but is the simplest solution. We could make the application delete $PROMETHEUS_MULTIPROC_DIR/*.db on startup. Conclusion So here we have it. An off-by-one (decrement to the count) can lead to an increment of 99. Who would have thought.\n","date":"2024-10-09T21:00:00-07:00","permalink":"https://hashimcolombowala.com/p/another-off-by-one-mostly-problem-and-prometheus-counter-spikes/","title":"Another Off-by-One (Mostly) Problem and Prometheus Counter Spikes"},{"content":"Imagine you\u0026rsquo;re writing a library to extend the prometheus python client and you needed to add some dynamic labels specific to the environment. Ideally you would add most of your labels at the collector end and avoid writing extensions.\nSay you need to inject some dynamic labels and you will need to at some point, we may want to extend the client.\nWhat are Generics? and why do we use them in python? Without sacrificing the inherent safety of a statically typed language, generic programming gives us primitives to declare “placeholder types” that allow us to focus less on the specific types that may be used or declared by other portions of the codebase, but rather focus on these higher-level patterns that can be consolidated into simpler declarations.\nPython has no built-in type checking. As long as a given Python program is syntactically valid, it will run, and issues like incompatible types will only surface at runtime. This forces the developer to ensure there is error handling in place to deal with such errors, and even with this, a common best practice is to use type hints combined with third-party linting tools to try to stay on top of issues like this.\nGenerics in my opinion is a programming stype for statically typed languages brought into python via type hints.\nSome code with comments 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 from prometheus_client import Counter as _PromCounter from prometheus_client import Histogram as _PromHistogram from typing import TypeVar, Generics # a new generic type, bound to two types _MetricsTypeT = TypeVar(\u0026#39;_MetricsTypeT\u0026#39;, bound=[_PromCounter, _PromHistogram]) # base class of type generic(child class to pass type in) class _MetricsBase(Generic[_MetricsTypeT]): def __init__(self, label_names: Iterable[str]): self.default_labels: Dict[str] = get_default_labels() self.all_label_names: list = list(label_names) + list(self.default_labels.keys()) self._parent_metric: _MetricsTypeT = None # Provides the label functionality def labels(self, *labelargs, **labelkwargs) -\u0026gt; _MetricsTypeT: if labelargs: labelargs += tuple(self.default_labels.values()) return cast(_MetricsTypeT, self._parent_metric.labels(*labelargs)) labelkwargs.update(self.default_labels) return cast(_MetricsTypeT, self._parent_metric.labels(**labelkwargs)) # child class passing in type to base class via generics class Counter(_MetricsBase[_PromCounter]): def __init__(self, name, documentation, labelnames=()): super().__init__(label_names=labelnames) self._parent_metric = _PromCounter( name=name, documentation=documentation, labelnames=self.all_label_names) Complete code here\nThe final world In my project, I used generics programing to type my base class. My base clase _MetricsBase accepts a Generic type, passed in by classes(Counter, Histogram) inheritering from it. The common method label returns the generic type passed in from the child class. When we use third party linters like mypy, we get some of the controll of statically typed languages with python\n","date":"2022-07-02T14:30:00Z","permalink":"https://hashimcolombowala.com/p/learning-generics-by-extending-the-prometheus-python-client/","title":"Learning Generics by extending the prometheus python client"},{"content":"To effectively monitor and understand the performance of your applications and infrastructure, having a well-designed metrics system is crucial. Here are the key requirements and components for building a reliable metrics system.\nRequirements for a Metrics System Multidimensional Data Model: The metrics system should support a multidimensional data model that can be sliced and diced along different dimensions as defined by the service (e.g., instance, service, endpoint, method). Operational Simplicity: The system should be easy to operate and maintain, minimizing overhead and complexity. Scalable Data Collection: The system must support scalable data collection and offer a decentralized architecture, allowing independent teams to set up their own monitoring servers. Powerful Query Language: A powerful query language should be available to leverage the data model for alerting and graphing, enabling precise insights into system performance. Client Libraries Client libraries play an essential role in the metrics system:\nThey handle details like thread safety, bookkeeping, and producing the Prometheus text exposition format in response to HTTP requests. Since metrics-based monitoring doesn\u0026rsquo;t track individual events, client library memory usage doesn\u0026rsquo;t increase with more events. Instead, memory usage depends on the number of metrics being tracked. Instrumentation To effectively monitor different types of services, appropriate instrumentation methods must be used. Here are three common types of services and how they should be instrumented:\nOnline-Serving Systems For online-serving systems, such as web services, the RED Method is used. This method involves tracking:\nRequests: The count of incoming requests. Errors: The count of failed requests. Duration: The latency or response time of requests. For example, a cache might track these metrics for both overall performance and for cache misses that need to be recalculated or fetched from a backend.\nOffline-Serving Systems Offline-serving systems, such as log processors, usually batch up work and consist of multiple stages in a pipeline with queues in between. These systems run continuously, which distinguishes them from batch jobs. The USE Method is used for these types of systems:\nUtilization: How much of the system\u0026rsquo;s capacity is in use (e.g., how much work is in progress). Saturation: The amount of queued work and how much work is currently being processed. Errors: Any errors encountered during processing. Batch Jobs Batch jobs are processes that run at scheduled intervals. The key metrics for batch jobs include:\nRun Time: How long it took for the job to complete. Stage Duration: How long each stage of the job took to complete. Success Time: The time at which the job last succeeded. Alerts can be set for when the job hasn\u0026rsquo;t succeeded within a certain time frame.\nIdempotency for Batch Jobs: Idempotency is an important concept for batch jobs. It means that performing an operation more than once has the same effect as performing it only once, which is crucial for reliability and preventing unintended side effects.\n","date":"2022-07-01T01:33:40Z","permalink":"https://hashimcolombowala.com/p/designing-a-metrics-system-notes/","title":"Designing a metrics system notes"},{"content":"This post will briefly compare and contrast the two most common authentication techniques: session-based and token-based authentication. Understanding the differences between these methods is crucial for selecting the right approach for your application\u0026rsquo;s needs.\nSession-Based Authentication In session-based authentication, the server creates a session for the user after the user logs in. The server generates a session id that is stored in memory or an external cache for horizontal scaling. The client stores the session ID in a cookie and sends it in the request header for every subsequent request.\n1 Cookie: JSESSIONID=ABAD1D The server can verify the user by comparing the session ID in the cookie against the session information stored in cache and respond accordingly.\nToken-Based Authentication In token-based authentication, the server generates a JWT (JSON Web Token) with a secret and sends it to the client. The client stores the JWT in a cookie or local browser memory. Subsequent requests made to the server include the JWT in the header:\n1 Authorization: Bearer eyJhbGciOiJIUzI1NiIXVCJ9TJV The server validates the JWT to verify the user and respond accordingly.\nSession vs. Token Authentication Tokens (JWT) Can work cross-origin across different domains. Downstream services can share tokens. JWT-based authentication scales well horizontally because tokens are stored on the client side. Provides integrity protection using a signature or MAC. Sessions Offers more control over sessions, as they are easier to invalidate. JWTs remain valid until their expiration date is reached. If a JWT contains a lot of data, it can slow down client requests. Session IDs are lightweight strings, making them more efficient. OAuth2 solves this issue by using short-lived access tokens and long-lived refresh tokens. Conclusion Most production services can work with either authentication model, so the choice depends on the use case. In fact, many systems use a hybrid model that combines both types of authentication, with the JWT being associated with a user session for user tracking.\n","date":"2021-06-18T12:33:40Z","permalink":"https://hashimcolombowala.com/p/comparing-session-based-and-token-based-authentication/","title":"Comparing Session-Based and Token-Based Authentication"},{"content":"When deploying a containerized application to a container management system like AWS Fargate, you tend to run your application from a shell script. Suppose your script looks like this\n1 2 3 set -o nounset set -x gunicorn --config config/gunicorn/$GUNICORN_CONFIG.py config.wsgi Here we are executing the gunicorn service with PID 1 when the container is deployed. Suppose we want to terminate the container with a docker stop \u0026lt;container_id\u0026gt;, the command will send a SIGTERM to the container. As the gunicorn process is PID 1, this signal is ignored.\nThe way to resolve this issue is to use exec before the command to start your application. The last line of the above shell script should be gunicorn --config config/gunicorn/$GUNICORN_CONFIG.py config.wsgi. A simple example is shown below\nt.py:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import signal import time got_signal = False def process_signal(signum, frame): global got_signal got_signal = True signal.signal(signal.SIGINT, process_signal) signal.signal(signal.SIGTERM, process_signal) while not got_signal: time.sleep(1) print(\u0026#34;looping...\u0026#34;) print(\u0026#34;Ended with signal.\u0026#34;) entry.sh:\n1 2 #!/usr/bin/env bash exec python ./t.py Dockerfile:\n1 2 3 4 5 FROM python:3.7 WORKDIR /usr/src/app COPY . . CMD [ \u0026#34;./entry.sh\u0026#34; ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 With the exec: $ docker run --rm -t signal looping... looping... looping... looping... looping... looping... Ended with signal. $ Without the exec: $ docker run --rm -t signal looping... looping... looping... looping... looping... looping... $ Both times, you can run docker stop in another window. If the container doesn’t stop within 10 seconds, then it’s killed.\nMost apps do not explicitely handle SIGTERM the way t.py does. If you replace t.py with\n1 2 3 4 5 6 7 import time while True: time.sleep(1) print(\u0026#34;looping...\u0026#34;) print(\u0026#34;Ended with signal.\u0026#34;) SIGINT will work with keyboard interrupt but docker stop does not because of PID=1 issue.\nIf we run docker with a --init option to force a non 1 PID, the docker stop works whether we use exec in the script or not.\nIn order to use this feature with AWS Fargate, make a small change to your AWS::ECS::TaskDefinition in your cloudformation as shown below\n1 2 3 4 5 6 ContainerDefinitions: - Name: \u0026lt;container_name\u0026gt; Image: \u0026lt;container_image\u0026gt; Essential: \u0026#34;true\u0026#34; linuxParameters: initProcessEnabled: \u0026#34;true\u0026#34; ","date":"2021-01-27T01:33:40Z","permalink":"https://hashimcolombowala.com/p/signalling-docker-containers/","title":"Signalling docker containers"},{"content":"In python3 all classes are new style classes, thus it is reasonable to refer to an objects type and its class interchangably\nType and Class 1 2 3 4 5 6 class Foo: pass \u0026gt;\u0026gt;\u0026gt; type(Foo) \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; Type is a metaclass of which classes are instances\nFoo is an instance of metaclass type type is an instance of type as well Type Metaclass A type metaclass is initialized with 3 arguments\nname: name of the class (name attribute) bases: a tuple of classnames that the class inherits from namespace: a dictionary contianing definitions of the class body (dict attribute of the class) Creating an abstract class manually with Metaclasses To understand metaclasses, we create an interface or abstract class implementation. Use from abc import ABC, abstractmethod when implementing something at work.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 # test.py # Decorator to add attribute to function def abstract_func(func): func.__isabstract__ = True return func # This is the metaclass inheriting from `type` class Interface(type): def __init__(self, name, bases, namespace): print(f\u0026#34;Init method initialized from {self}\u0026#34;) class_methods = getattr(self, \u0026#39;all_methods\u0026#39;) for base in bases: required_methods = getattr(base, \u0026#39;abstract_methods\u0026#39;) for method in required_methods: if method not in class_methods: msg = f\u0026#34;\u0026#34;\u0026#34;Can\u0026#39;t create abstract class {name}! {name} must implement abstract method {method} of class {base}!\u0026#34;\u0026#34;\u0026#34; raise TypeError(msg) def __new__(cls, name, bases, namespace): namespace[\u0026#39;abstract_methods\u0026#39;] = Interface._get_abstract_methods(namespace) namespace[\u0026#39;all_methods\u0026#39;] = Interface._get_all_methods(namespace) cls = super().__new__(cls, name, bases, namespace) return cls def _get_abstract_methods(namespace): ret = [] for name, val in namespace.items(): if callable(val) and getattr(val, \u0026#39;__isabstract__\u0026#39;, False): ret.append(name) return ret def _get_all_methods(namespace): ret = [] for name, val in namespace.items(): if callable(val): ret.append(name) return ret # the __calls__() function calls the __new__() and __init__() methods of the metaclass class NetworkInterface(metaclass=Interface): @abstract_func def connect(self): pass @abstract_func def transfer(self): pass # The object of this class will not be created # because of missing abstract method class TestNetwork(NetworkInterface): def __init__(self): print(f\u0026#34;Init method initialized from {self}\u0026#34;) def connect(self): pass # def transfer(self): # pass c = TestNetwork() \u0026gt;\u0026gt;\u0026gt; TypeError: Can\u0026#39;t create abstract class TestNetwork! TestNetwork must implement abstract method transfer of class \u0026lt;class \u0026#39;__main__.NetworkInterface\u0026#39;\u0026gt;! After uncommenting the method \u0026gt;\u0026gt;\u0026gt; python3 test.py Init method initialized from \u0026lt;class \u0026#39;__main__.NetworkInterface\u0026#39;\u0026gt; Init method initialized from \u0026lt;class \u0026#39;__main__.TestNetwork\u0026#39;\u0026gt; Init method called from \u0026lt;__main__.TestNetwork object at 0x7fc5e7659350\u0026gt; When we initialize TestNetwork, the following happens\nThe interface init method is called twice. Once when creating the NetworkInterface and TestNetwork class from the metaclass blueprint. In the Interface init method, we iterate over the list of abstract methods in the parent class and make sure each one is present in the current class. If we don\u0026rsquo;t find a method in the class with the same name, we raise an exception ","date":"2020-12-28T01:33:40Z","permalink":"https://hashimcolombowala.com/p/abstract-classes-using-metaclasses/","title":"Abstract classes using Metaclasses"},{"content":"The Go compiler can compile the Go source go with different go specs. Fo example, if you have installed go 1.14, you can compile your source with Go spec 1.13.\nThe rules for which version of the Go spec used during compilation appear to be If your source code is stored within the GOPATH (or you have disabled modules with GO111MODULE=off) then the version of the Go spec used to compile matches the version of the compiler you are using. ie if you have go 1.13(GOROOT points to your go installation) installed then the go spec used will be 1.13 If your source code is outside the GOPATH(or GO111MODULE=on), then the go tool will take the version from the go.mod file If no go.mod file is provided, then same as point 1 If you are in module mode(see point 2) and no version is specified in the go.mod file, then go 1.13 is used by default The last point is interesting.\nReference: https://www.jetbrains.com/help/go/configuring-goroot-and-gopath.html\n","date":"2020-06-20T16:00:00-07:00","permalink":"https://hashimcolombowala.com/p/the-go-speccompiling/","title":"The Go Spec(compiling)"},{"content":"Every once in a while, it\u0026rsquo;s good to revisit the principles that guide Python\u0026rsquo;s design and philosophy. One of the most iconic expressions of these principles is \u0026ldquo;The Zen of Python\u0026rdquo; by Tim Peters. If you haven\u0026rsquo;t read it in a while, it might be worth taking a moment to appreciate the simplicity and wisdom it contains.\nTo view \u0026ldquo;The Zen of Python,\u0026rdquo; open a Python interpreter and enter:\n1 \u0026gt;\u0026gt;\u0026gt; import this Here\u0026rsquo;s what you\u0026rsquo;ll see:\nThe Zen of Python, by Tim Peters\nBeautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\u0026rsquo;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one—and preferably only one—obvious way to do it. Although that way may not be obvious at first unless you\u0026rsquo;re Dutch. Now is better than never. Although never is often better than right now. If the implementation is hard to explain, it\u0026rsquo;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea—let\u0026rsquo;s do more of those! The Zen of Python encapsulates the values that make Python such a delightful programming language. It\u0026rsquo;s about simplicity, clarity, and pragmatism. Whether you\u0026rsquo;re new to Python or a seasoned developer, keeping these principles in mind can help guide you to write better, more maintainable code.\n","date":"2020-04-07T18:54:00-07:00","permalink":"https://hashimcolombowala.com/p/the-zen-of-python/","title":"The Zen of Python"},{"content":"Python decorators are a powerful feature that allow you to extend the functionality of existing functions or methods in a reusable and elegant way. They are especially useful for tasks that you may need to apply repeatedly to different functions. Below are some common use cases for Python decorators.\n1. Timing Functions Timing functions are a classic example of using decorators, often featured as the first example on StackOverflow. These decorators help measure how long a function takes to execute, making it easy to identify performance bottlenecks. You can also use a context manager to time functions, which is a good way to learn how __enter__ and __exit__ methods work in Python.\n2. Retrying Functions Sometimes, a function may fail due to transient errors (e.g., network issues). A retrying decorator can be used to attempt the function again after a failure, with optional exponential backoff to increase the wait time between retries. Bonus points if you implement increasing wait times to avoid overwhelming the system.\n3. Setting a Maximum Execution Time If you have a function that runs in a loop or is amenable to timeout, a decorator can enforce a maximum execution time. This can be useful when you want to avoid infinite loops or long-running operations that may block other processes.\n4. Logging Functions Logging is an essential part of debugging and monitoring applications. There are numerous types of logging decorators, each serving different purposes. Often, these decorators require a decorator factory or class to accommodate parameters like a logger instance. You can use decorators to log function calls, inputs, outputs, and exceptions.\n5. Simple Debugging A simple debugging decorator can print the function\u0026rsquo;s inputs and outputs, either to a log file or just to the console. This is especially useful during development when trying to trace issues in code.\n6. Validating Function Inputs Python is typically \u0026ldquo;Easier to Ask for Forgiveness than Permission\u0026rdquo; (EAFP), but there are times when input validation is necessary. A decorator can validate function inputs, ensuring they meet certain criteria. You can also use this to modify inputs—for instance, adding http:// to a URL if the user forgets to include it.\n7. Validating Function Outputs In addition to input validation, decorators can also validate function outputs. For example, you can enforce output constraints, such as ensuring the value is within a specific range.\n8. Waiting/Rate-Limiting When interacting with a web service, it\u0026rsquo;s often crucial to rate-limit your requests to avoid being banned or overwhelming the server. A waiting or rate-limiting decorator can add delays between function calls, ensuring compliance with rate limits.\n9. Caching/Memoization If you have a pure or idempotent function that\u0026rsquo;s computationally expensive and runs multiple times with the same inputs, a caching decorator (e.g., functools.lru_cache) can save results for future use. This is particularly useful in functional programming paradigms to improve efficiency by avoiding redundant calculations.\n10. Handling Database Transactions Gracefully When working with databases, it\u0026rsquo;s important to handle transactions properly. A decorator can ensure that transactions are committed when successful or rolled back when an exception occurs. Although many DB-API libraries offer built-in ways to manage transactions, a decorator can provide a consistent and reusable approach.\n11. Synchronization In multithreading or multiprocessing applications, decorators can be used for synchronization—acquiring and releasing locks. This helps prevent race conditions by ensuring only one thread or process can access a critical section at a time.\n12. Authentication Authentication is a common use case in web frameworks, where decorators are used to ensure that a user is logged in before they can access a certain resource or perform an action. This is a convenient way to add access control across multiple endpoints in a web application.\nConclusion Decorators offer a powerful, reusable, and elegant way to extend the behavior of your functions. From timing functions and retry mechanisms to logging and input validation, decorators help reduce repetitive code while maintaining consistency across your application.\n","date":"2020-04-04T15:54:35-07:00","permalink":"https://hashimcolombowala.com/p/python-decorator-common-use-cases/","title":"Python decorator common use cases"}]