[{"content":"Overview Recently, the observability team was tasked with a latency reporting project. The key features included:\nDaily and monthly latency aggregations Support for various quantiles A reasonable data delay SLO (24 hours) and more. For the purpose of this blog, we will focus on the data processing aspect. We needed a system that could read latency data from a Kafka stream, partition the data by various attributes such as endpoints, and precompute hourly, daily, and weekly quantiles (e.g., p50, p99).\nWhy did we choose Flink? We chose Flink because of several key features:\nStateful processing: It allows us to maintain state across events. Built-in windowing support: Flink offers windowing functions (e.g., tumbling, sliding) that divide the data stream into finite subsets (windows) based on time or count. Scalability: It can handle high-throughput data streams, such as those from Kafka. Fault tolerance: Through state snapshots and checkpointing, Flink ensures resilience. Flink Playground Naturally, besides going through the flink documentation [insert link], I decided go to my usual playground of choice. Docker. Below is the architecture. Checkout the code and docker-compose.yml for more information. Below is the basic architecture.\ngraph TD; P[\"Python App (produces latency data)\"] --\u003e A[\"Kafka Broker (latency topic)\"] %% Python app produces random latency data and sends it to the Kafka broker A --\u003e B[\"Flink Pipeline\"] %% Kafka Broker receives latency messages produced by the Python app, which are consumed by the Flink pipeline for processing B --\u003e C[\"1-Minute Tumbling Window\"] %% The Flink Pipeline splits the data stream into a 1-minute tumbling window for short-term aggregation B --\u003e D[\"1-Hour Tumbling Window\"] %% The Flink Pipeline also splits the data stream into a 1-hour tumbling window for long-term aggregation C --\u003e E[\"P50/P99 Aggregation\"] %% Once the 1-minute window is over, the data is processed to calculate P50 and P99 latency metrics D --\u003e F[\"P50/P99 Aggregation\"] %% Once the 1-hour window is over, the data is also processed to calculate P50 and P99 latency metrics E --\u003e G[\"Storage System (P50/P99 results)\"] %% The aggregated results from the 1-minute window are sent to a storage system for further use F --\u003e G[\"Storage System (P50/P99 results)\"] %% The aggregated results from the 1-hour window are also sent to a storage system for further use So what impressed me? Flink\u0026rsquo;s brilliance lies in its ability to effortlessly handle the complexity of distributed stream processing with concise, expressive code. take this one line for example\n1 2 3 4 5 val oneMinuteWindows = latencyStream .keyBy({ it.timestamp / 60000 }, TypeInformation.of(object : TypeHint\u0026lt;Long\u0026gt;() {})) .window(TumblingProcessingTimeWindows.of(Time.minutes(1))) .process(LatencyAggregateProcessFunction()) .returns(TypeInformation.of(object : TypeHint\u0026lt;AggregateResult\u0026gt;() {})) Flink simplifies real-time data processing by abstracting away many low-level concerns while giving developers precise control over time, state, and computation. In this single line, Flink allows us to:\nPartition the stream: The keyBy function logically partitions the incoming latencyStream based on timestamps, so each partition processes its events independently, enabling scalability in distributed environments. Windowing: Using TumblingProcessingTimeWindows.of(Time.minutes(1)), Flink groups the data into 1-minute windows based on event processing time, making it easy to aggregate data over defined time intervals. Custom processing: The process function applies a user-defined LatencyAggregateProcessFunction for calculating latency metrics, allowing custom logic to be executed for each window. Type safety and efficiency: Using Flink’s TypeHint, we ensure type safety and help optimize serialization and deserialization in distributed environments, which enhances performance. This one-liner hides a ton of complexity — from fault tolerance to scaling — and Flink’s ability to marry simplicity with powerful distributed processing is where it shines. In just a few lines of code, you have a resilient, scalable, and efficient stream processing pipeline!\n","description":"","id":0,"section":"posts","tags":["flink","kotlin"],"title":"What is flink and why should we care about it","uri":"https://hashimcolombowala.com/posts/2024-10-08-kafka-flink/"},{"content":"Background Imagine you\u0026rsquo;re writing a library to extend the prometheus python client and you needed to add some dynamic labels specific to the environment. Ideally you would add most of your labels at the collector end and avoid writing extensions.\nSay you need to inject some dynamic labels and you will need to at some point, we may want to extend the client.\nWhat are Generics? and why do we use them in python? Without sacrificing the inherent safety of a statically typed language, generic programming gives us primitives to declare “placeholder types” that allow us to focus less on the specific types that may be used or declared by other portions of the codebase, but rather focus on these higher-level patterns that can be consolidated into simpler declarations.\nPython has no built-in type checking. As long as a given Python program is syntactically valid, it will run, and issues like incompatible types will only surface at runtime. This forces the developer to ensure there is error handling in place to deal with such errors, and even with this, a common best practice is to use type hints combined with third-party linting tools to try to stay on top of issues like this.\nGenerics in my opinion is a programming stype for statically typed languages brought into python via type hints.\nSome code with comments from prometheus_client import Counter as _PromCounter from prometheus_client import Histogram as _PromHistogram from typing import TypeVar, Generics # a new generic type, bound to two types _MetricsTypeT = TypeVar(\u0026#39;_MetricsTypeT\u0026#39;, bound=[_PromCounter, _PromHistogram]) # base class of type generic(child class to pass type in) class _MetricsBase(Generic[_MetricsTypeT]): def __init__(self, label_names: Iterable[str]): self.default_labels: Dict[str] = get_default_labels() self.all_label_names: list = list(label_names) + list(self.default_labels.keys()) self._parent_metric: _MetricsTypeT = None # Provides the label functionality def labels(self, *labelargs, **labelkwargs) -\u0026gt; _MetricsTypeT: if labelargs: labelargs += tuple(self.default_labels.values()) return cast(_MetricsTypeT, self._parent_metric.labels(*labelargs)) labelkwargs.update(self.default_labels) return cast(_MetricsTypeT, self._parent_metric.labels(**labelkwargs)) # child class passing in type to base class via generics class Counter(_MetricsBase[_PromCounter]): def __init__(self, name, documentation, labelnames=()): super().__init__(label_names=labelnames) self._parent_metric = _PromCounter( name=name, documentation=documentation, labelnames=self.all_label_names) Complete code here\nThe final world In my project, I used generics programing to type my base class. My base clase _MetricsBase accepts a Generic type, passed in by classes(Counter, Histogram) inheritering from it. The common method label returns the generic type passed in from the child class. When we use third party linters like mypy, we get some of the controll of statically typed languages with python\n","description":"","id":1,"section":"posts","tags":["python","o11y"],"title":"Learning Generics by extending the prometheus python client","uri":"https://hashimcolombowala.com/posts/generics/"},{"content":"Requirements for metrics system Multidimensional data model which can be sliced and diced along different dimensions as defined by the service(example: instance, service, endpoint, method) Operational simplicity Scalable data collection and decentralized architecture, so that independent teams can setup independent monitoring servers A powerful query language that leverages the data model for alerting and graphing Client libraries Client libraries take care of all the nitty gritty details like thread-safety, bookkeeping and producing the Prometheus text exposition format in response to HTTP request. As metrics-based monitoring does not track individual events, client library memory usage does not increase the more events you have. Rather, memory is related to the number of metrics you have. Instrumentation There are 3 types of services\nOnline-serving systems: RED method, count of requests, errors and duration(latency). “synchronous function calls, and benefit from the same metrics of requests, latency, and errors. For a cache you would want these metrics both for the cache overall and the cache misses that then need to calculate the result or request it from a backend”\nOffline-serving systems: eg log processor. They usually batch up work, have multiple stages in a pipeline with queues between them. These systems run continuously which differentiates them from batch jobs. USE method.\nUtilization: How full your service is.how much work is in progress. How fast are you processing items\nSaturation: Amount of queued worked and how much work is in progress\nErrors: Self explanatory\nBatch jobs: How long it took to run, how long each stage took and the time at which the job last succeeded. Alerts for when the job hasn’t succeeded recently enough\nIdempotency for batch jobs: Idempotency is the property that if you do something more than once, it has the same result as if it was only done once.\n","description":"","id":2,"section":"posts","tags":["python","o11y"],"title":"Designing a metrics system notes","uri":"https://hashimcolombowala.com/posts/metrics/"},{"content":"This post will compare and contrast two most common authentication techniques breifly.\nSession Based Authentication In this scenario, the server will create a session for the user after the user logs in. The server creates a session id which is stored in memory or in an external cache for horizontal scaling. The client stores the session id in a cookie and sends the it in the request header for every subsequent request.\n1 Cookie: JSESSIONID=ABAD1D The server can then verify the user by comparing the server id in the cookie against the session information stored in cache and responds accordingly.\nToken Authentication The server will generate a JWT(Json Web Token) with a secret and send it to the client. The client stores the JWT in a cookie or local brower memory. Subsequent requests made to server have the JWT in the header\n1 Authorization: Bearer eyJhbGciOiJIUzI1NiIXVCJ9TJV The server validates the JWT to verify the user and respond accordingly.\nSession vs Token Tokens(JWT)\nCan work cross orgin across different domains. Downstream services can share tokens JWT based authentication scales well horizontally because tokens are stored on the client side Integrity protection by using a signature or MAC Sessions\nMore control over sessions as it is easier to invalidate. JWTs are valid till the expiration date is reached. If the JWT is encoded with a lot of data, it can slow down client requests. Session ids are just a string and very lightweight. Oauth2 solves this by having short lived access tokens and long lived refresh tokens Conclusion In conclusion I would like to say that most production services can work with either models, so it depends on the use case. In fact many systems use a hybrid model with both types of authentication as well as combined model in which the JWT is associated with a user session for user tracking.\n","description":"","id":3,"section":"posts","tags":["python"],"title":"Authentication - Session or Token?","uri":"https://hashimcolombowala.com/posts/auth/"},{"content":"When deploying a containerized application to a container management system like AWS Fargate, you tend to run your application from a shell script. Suppose your script looks like this\n1 2 3 set -o nounset set -x gunicorn --config config/gunicorn/$GUNICORN_CONFIG.py config.wsgi Here we are executing the gunicorn service with PID 1 when the container is deployed. Suppose we want to terminate the container with a docker stop \u0026lt;container_id\u0026gt;, the command will send a SIGTERM to the container. As the gunicorn process is PID 1, this signal is ignored.\nThe way to resolve this issue is to use exec before the command to start your application. The last line of the above shell script should be gunicorn --config config/gunicorn/$GUNICORN_CONFIG.py config.wsgi. A simple example is shown below\nt.py:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 import signal import time got_signal = False def process_signal(signum, frame): global got_signal got_signal = True signal.signal(signal.SIGINT, process_signal) signal.signal(signal.SIGTERM, process_signal) while not got_signal: time.sleep(1) print(\u0026#34;looping...\u0026#34;) print(\u0026#34;Ended with signal.\u0026#34;) entry.sh:\n1 2 #!/usr/bin/env bash exec python ./t.py Dockerfile:\n1 2 3 4 5 FROM python:3.7 WORKDIR /usr/src/app COPY . . CMD [ \u0026#34;./entry.sh\u0026#34; ] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 With the exec: $ docker run --rm -t signal looping... looping... looping... looping... looping... looping... Ended with signal. $ Without the exec: $ docker run --rm -t signal looping... looping... looping... looping... looping... looping... $ Both times, you can run docker stop in another window. If the container doesn’t stop within 10 seconds, then it’s killed.\nMost apps do not explicitely handle SIGTERM the way t.py does. If you replace t.py with\n1 2 3 4 5 6 7 import time while True: time.sleep(1) print(\u0026#34;looping...\u0026#34;) print(\u0026#34;Ended with signal.\u0026#34;) SIGINT will work with keyboard interrupt but docker stop does not because of PID=1 issue.\nIf we run docker with a --init option to force a non 1 PID, the docker stop works whether we use exec in the script or not.\nIn order to use this feature with AWS Fargate, make a small change to your AWS::ECS::TaskDefinition in your cloudformation as shown below\n1 2 3 4 5 6 ContainerDefinitions: - Name: \u0026lt;container_name\u0026gt; Image: \u0026lt;container_image\u0026gt; Essential: \u0026#34;true\u0026#34; linuxParameters: initProcessEnabled: \u0026#34;true\u0026#34; ","description":"","id":4,"section":"posts","tags":["python","docker"],"title":"Signalling docker containers","uri":"https://hashimcolombowala.com/posts/signal/"},{"content":"In python3 all classes are new style classes, thus it is reasonable to refer to an objects type and its class interchangably\nType and Class 1 2 3 4 5 6 class Foo: pass \u0026gt;\u0026gt;\u0026gt; type(Foo) \u0026lt;class \u0026#39;type\u0026#39;\u0026gt; Type is a metaclass of which classes are instances\nFoo is an instance of metaclass type type is an instance of type as well Type Metaclass A type metaclass is initialized with 3 arguments\nname: name of the class (name attribute) bases: a tuple of classnames that the class inherits from namespace: a dictionary contianing definitions of the class body (dict attribute of the class) Creating an abstract class manually with Metaclasses To understand metaclasses, we create an interface or abstract class implementation. Use from abc import ABC, abstractmethod when implementing something at work.\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 # test.py # Decorator to add attribute to function def abstract_func(func): func.__isabstract__ = True return func # This is the metaclass inheriting from `type` class Interface(type): def __init__(self, name, bases, namespace): print(f\u0026#34;Init method initialized from {self}\u0026#34;) class_methods = getattr(self, \u0026#39;all_methods\u0026#39;) for base in bases: required_methods = getattr(base, \u0026#39;abstract_methods\u0026#39;) for method in required_methods: if method not in class_methods: msg = f\u0026#34;\u0026#34;\u0026#34;Can\u0026#39;t create abstract class {name}! {name} must implement abstract method {method} of class {base}!\u0026#34;\u0026#34;\u0026#34; raise TypeError(msg) def __new__(cls, name, bases, namespace): namespace[\u0026#39;abstract_methods\u0026#39;] = Interface._get_abstract_methods(namespace) namespace[\u0026#39;all_methods\u0026#39;] = Interface._get_all_methods(namespace) cls = super().__new__(cls, name, bases, namespace) return cls def _get_abstract_methods(namespace): ret = [] for name, val in namespace.items(): if callable(val) and getattr(val, \u0026#39;__isabstract__\u0026#39;, False): ret.append(name) return ret def _get_all_methods(namespace): ret = [] for name, val in namespace.items(): if callable(val): ret.append(name) return ret # the __calls__() function calls the __new__() and __init__() methods of the metaclass class NetworkInterface(metaclass=Interface): @abstract_func def connect(self): pass @abstract_func def transfer(self): pass # The object of this class will not be created # because of missing abstract method class TestNetwork(NetworkInterface): def __init__(self): print(f\u0026#34;Init method initialized from {self}\u0026#34;) def connect(self): pass # def transfer(self): # pass c = TestNetwork() \u0026gt;\u0026gt;\u0026gt; TypeError: Can\u0026#39;t create abstract class TestNetwork! TestNetwork must implement abstract method transfer of class \u0026lt;class \u0026#39;__main__.NetworkInterface\u0026#39;\u0026gt;! After uncommenting the method \u0026gt;\u0026gt;\u0026gt; python3 test.py Init method initialized from \u0026lt;class \u0026#39;__main__.NetworkInterface\u0026#39;\u0026gt; Init method initialized from \u0026lt;class \u0026#39;__main__.TestNetwork\u0026#39;\u0026gt; Init method called from \u0026lt;__main__.TestNetwork object at 0x7fc5e7659350\u0026gt; When we initialize TestNetwork, the following happens\nThe interface init method is called twice. Once when creating the NetworkInterface and TestNetwork class from the metaclass blueprint. In the Interface init method, we iterate over the list of abstract methods in the parent class and make sure each one is present in the current class. If we don\u0026rsquo;t find a method in the class with the same name, we raise an exception ","description":"","id":5,"section":"posts","tags":["python"],"title":"Abstract classes using Metaclasses","uri":"https://hashimcolombowala.com/posts/2020-12-28-python-metaclasses/"},{"content":"The Go compiler can compile the Go source go with different go specs. Fo example, if you have installed go 1.14, you can compile your source with Go spec 1.13.\nThe rules for which version of the Go spec used during compilation appear to be If your source code is stored within the GOPATH (or you have disabled modules with GO111MODULE=off) then the version of the Go spec used to compile matches the version of the compiler you are using. ie if you have go 1.13(GOROOT points to your go installation) installed then the go spec used will be 1.13 If your source code is outside the GOPATH(or GO111MODULE=on), then the go tool will take the version from the go.mod file If no go.mod file is provided, then same as point 1 If you are in module mode(see point 2) and no version is specified in the go.mod file, then go 1.13 is used by default The last point is interesting.\nReference: https://www.jetbrains.com/help/go/configuring-goroot-and-gopath.html\n","description":"","id":6,"section":"posts","tags":["go"],"title":"The Go Spec(compiling)","uri":"https://hashimcolombowala.com/posts/2020-06-20-the-go-spec/"},{"content":"Words of wisdom from the python interactive shell. Always good to read it once in a while and appreciate the zen of python.\nOpen the python interpretor and enter import this\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 \u0026gt;\u0026gt;\u0026gt; import this The Zen of Python, by Tim Peters Beautiful is better than ugly. Explicit is better than implicit. Simple is better than complex. Complex is better than complicated. Flat is better than nested. Sparse is better than dense. Readability counts. Special cases aren\\\u0026#39;t special enough to break the rules. Although practicality beats purity. Errors should never pass silently. Unless explicitly silenced. In the face of ambiguity, refuse the temptation to guess. There should be one-- and preferably only one --obvious way to do it. Although that way may not be obvious at first unless you\\\u0026#39;re Dutch. Now is better than never. Although never is often better than *right* now. If the implementation is hard to explain, it\\\u0026#39;s a bad idea. If the implementation is easy to explain, it may be a good idea. Namespaces are one honking great idea -- let\\\u0026#39;s do more of those! ","description":"","id":7,"section":"posts","tags":null,"title":"The Zen of Python","uri":"https://hashimcolombowala.com/posts/2020-04-17-zen-of-python/"},{"content":"\nMy journey involves finishing a masters degree in Electrical Engineering at USC to working as a Sr. Software Engineer in the Observability space at a Fintech company in the Bay Area of California.\nOutside of tech, I enjoy hiking and biking with my better half, get the heart thumping with some crossfit and follow sports such as cricket, football(British) and tennis. I am big fan of Liverpool Football Club and its manager Jurgen Klopp \u0026ldquo;The normal one\u0026rdquo;(Now former manager).\n","description":"","id":8,"section":"","tags":null,"title":"","uri":"https://hashimcolombowala.com/about/"},{"content":"Hashim Colombowala Core experience Building and deploying containerized applications to AWS EKS/Fargate. Core knowledge of AWS services and infrastructure. Observability Systems: Logging (ELK), Metrics(Prometheus) and Tracing(OTEL). Library support for python and kotlin services with web, rpc, celery, batch and kafka components. Deployment to AWS EKS Kubernetes. Building scalable APIs and data models Experience with software release management. Highly knowledgeable with CI/CD systems like Buildkite, Jenkins and Drone. Technical Skills Toolset Languages: Python, Kotlin, Golang, Bash, Groovy\nBuild tools: Ansible, Packer, Terraform, Terragrunt\nPlatforms: Docker, Kubernetes, Kustomize\nAWS: EKS, Fargate, Kinesis, Secrets Management, Lambda, IAM, Networking, Dynamo, RDS and others\nBuild Systems: Jenkins, Drone, Buildkite\nELK Stack,\nOpentelemetry, Prometheus\nWork Experience Affirm, San Francisco, CA Sept \u0026lsquo;21 - Present Senior Software Engineer (Observability) Part of the team responsible for building olly v3 metrics and tracing. Existing metrics system built on top of ELK stack had difficulties scaling due to lack of aggregation and dramatic increase in volume of metrics due to onboarding of high profile customers.\nv3 metrics involved migrating our python and kotlin services to v3 based metrics system and opentelemetry tracing to improve performance and gain further insight into our services\n23andMe, Sunnyvale, CA May ‘18 - Sept \u0026lsquo;21 Software Engineer (Application Infrastructure) Part of the tools team responsible for building an opinionated tool for deploying containers to ECS Fargate on top of our core AWS infrastructure. Developed and deployed internal API services previously embedded in the monolith Django app using drf and various AWS services. Automated deployment of CI systems like Jenkins and Drone to AWS using technologies like docker and CloudFormation and streamlined the update process. Streamlined and automated various parts of the release process for deploying our\nmain products^ based^ on feedback from various release managers.^ APB Inc, Los Angeles, CA June‘13 – May‘18 Sr. Systems Engineer and ERP Systems Analyst Linux App Servers: installation, configuration, automation and maintenance (Puppet, Bash) Windows systems: AD, group policy, exchange, disaster recovery. Responsible for capacity planning, automation, network resource usage policies and evaluation of business and operational systems and user needs.\nAchievements: Designed and implemented the migration of physical DR application servers to AWS EC2 instances. Developed internal web apps for inventory and billing using Django. Education and Certifications Udacity Self Driving Car: Computer Vision nanodegree Nov ‘2017 AWS Certified Solutions Architect May ‘16 M.S., Electrical Engineering | University of Southern California Aug ‘07 - May ‘09 ","description":"","id":9,"section":"","tags":null,"title":"","uri":"https://hashimcolombowala.com/resume/"}]